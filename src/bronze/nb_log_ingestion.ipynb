{"cells":[{"cell_type":"code","source":["# ============================================================\n","# Notebook : nb_log_ingestion (v2 – Fabric clean)\n","# Rôle : écrire un log \"run-level\" dans tech_ingestion_log\n","# ============================================================\n","\n","# PARAMETER_CELL\n","# Paramètres fournis par le pipeline Fabric (Base parameters)\n","exec_date = \"\"              # ex: \"2025-12-02\" (format yyyy-MM-dd)\n","entity = \"\"                 # ex: \"FX\", \"CUSTOMER\"\n","total_files = \"0\"           # v_TotalFiles (source brute)\n","processed_files = \"0\"       # v_ProcessedFiles\n","failed_files = \"0\"          # v_FailedFiles\n","status = \"\"                 # SUCCESS / PARTIAL / FAILED / NO_DATA\n","ingestion_mode = \"\"       # FULL / INCR\n","total_candidate_files = \"0\" # v_TotalCandidateFiles (après filtre incrémental)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"f87c1f50-ec93-498c-8a74-f23e07030ffb"},{"cell_type":"code","source":["# ============================================================\n","# Cellule 2 – Imports, log des paramètres et cast\n","# ============================================================\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","from pyspark.sql.utils import AnalysisException\n","from notebookutils import mssparkutils\n","from datetime import datetime\n","\n","print(\"== nb_log_ingestion v3 ==\")\n","print(f\"exec_date             = {exec_date}\")\n","print(f\"entity                = {entity}\")\n","print(f\"total_files (raw)     = {total_files}\")\n","print(f\"processed_files (raw) = {processed_files}\")\n","print(f\"failed_files (raw)    = {failed_files}\")\n","print(f\"status                = {status}\")\n","print(f\"ingestion_mode      = {ingestion_mode}\")\n","print(f\"total_candidate_files = {total_candidate_files}\")\n","\n","# Sécurisation minimale\n","if not entity:\n","    print(\"ERREUR : entity manquant → aucun log écrit.\")\n","    mssparkutils.notebook.exit(\"NO_LOG\")\n","\n","if not status:\n","    status = \"UNKNOWN\"\n","\n","if not ingestion_mode:\n","    ingestion_mode = \"UNKNOWN\"\n","\n","# Cast des entiers\n","try:\n","    total_source_files_int = int(total_files)\n","except Exception:\n","    total_source_files_int = -1\n","\n","try:\n","    processed_files_int = int(processed_files)\n","except Exception:\n","    processed_files_int = -1\n","\n","try:\n","    failed_files_int = int(failed_files)\n","except Exception:\n","    failed_files_int = -1\n","\n","try:\n","    total_candidate_files_int = int(total_candidate_files)\n","except Exception:\n","    total_candidate_files_int = -1\n","\n","# Cast de la date d'exécution en vrai Date Python\n","try:\n","    exec_date_py = datetime.strptime(exec_date, \"%Y-%m-%d\").date()\n","except Exception:\n","    # Si problème de format, on met la date du jour\n","    exec_date_py = datetime.utcnow().date()\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"bc1b4db8-08df-472e-bc6a-0e85d91b6eb7","normalized_state":"finished","queued_time":"2025-12-01T08:14:59.4116177Z","session_start_time":null,"execution_start_time":"2025-12-01T08:15:07.9925155Z","execution_finish_time":"2025-12-01T08:15:31.5299894Z","parent_msg_id":"af1e3f4b-6820-4625-841b-5ba162418c75"},"text/plain":"StatementMeta(, bc1b4db8-08df-472e-bc6a-0e85d91b6eb7, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"aa2832d2-880b-442f-9651-3618e5bb0998"},{"cell_type":"code","source":["from pyspark.sql import Row\n","\n","schema = StructType([\n","    StructField(\"exec_date\", DateType(), False),\n","    StructField(\"entity\", StringType(), False),\n","    StructField(\"ingestion_mode\", StringType(), True),\n","    StructField(\"total_source_files\", IntegerType(), True),\n","    StructField(\"total_candidate_files\", IntegerType(), True),\n","    StructField(\"processed_files\", IntegerType(), True),\n","    StructField(\"failed_files\", IntegerType(), True),\n","    StructField(\"status\", StringType(), True),\n","    StructField(\"log_timestamp\", TimestampType(), True),\n","])\n","\n","row = Row(\n","    exec_date=exec_date_py,\n","    entity=entity,\n","    ingestion_mode=ingestion_mode,\n","    total_source_files=total_source_files_int,\n","    total_candidate_files=total_candidate_files_int,\n","    processed_files=processed_files_int,\n","    failed_files=failed_files_int,\n","    status=status,\n","    log_timestamp=datetime.utcnow()\n",")\n","\n","df_stage = spark.createDataFrame([row], schema)\n","\n","print(\"Schéma df_stage (tech_ingestion_log) :\")\n","df_stage.printSchema()\n","display(df_stage)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"403996ad-a4df-4d12-b280-2ceceda0ec3b"},{"cell_type":"code","source":["# ============================================================\n","# Cellule 4 – Création de la table tech_ingestion_log si besoin\n","# ============================================================\n","\n","try:\n","    spark.table(\"tech_ingestion_log\")\n","    print(\"Table tech_ingestion_log déjà existante.\")\n","except AnalysisException:\n","    print(\"Table tech_ingestion_log absente, création en cours...\")\n","    (\n","        df_stage.limit(0)\n","        .write\n","        .format(\"delta\")\n","        .mode(\"overwrite\")\n","        .saveAsTable(\"tech_ingestion_log\")\n","    )\n","    print(\"Table tech_ingestion_log créée.\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"edff8a02-cab1-41ba-a438-d70d484d2448"},{"cell_type":"code","source":["# ============================================================\n","# Cellule 5 – Écriture append dans tech_ingestion_log\n","# ============================================================\n","(\n","    df_stage\n","    .write\n","    .format(\"delta\")\n","    .mode(\"append\")\n","    .saveAsTable(\"tech_ingestion_log\")\n",")\n","\n","print(\"Ligne de log ajoutée dans tech_ingestion_log.\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6459624c-05a7-47d1-937a-7212db336f7b"},{"cell_type":"code","source":["# ============================================================\n","# Cellule 6 – Retour structuré au pipeline (facultatif mais recommandé)\n","# ============================================================\n","import json\n","\n","result = {\n","    \"exec_date\": str(exec_date_py),\n","    \"entity\": entity,\n","    \"ingestion_mode\": ingestion_mode,\n","    \"total_source_files\": total_source_files_int,\n","    \"total_candidate_files\": total_candidate_files_int,\n","    \"processed_files\": processed_files_int,\n","    \"failed_files\": failed_files_int,\n","    \"status\": status,\n","    \"log_timestamp\": datetime.utcnow().isoformat()\n","}\n","\n","print(\"== Résultat renvoyé au pipeline (nb_log_ingestion v3) ==\")\n","print(json.dumps(result, indent=2))\n","\n","mssparkutils.notebook.exit(json.dumps(result))\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f7dea57b-6ed6-4b42-b3c6-9e2742198310"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}