{"cells":[{"cell_type":"code","source":["# nb_validate_bronze_schema_registry\n","# Fabric Notebook (PySpark)\n","# Prereq: Notebook attached to Lakehouse lh_wm_core\n","\n","from datetime import datetime, timezone\n","import json\n","import re\n","\n","try:\n","    import yaml  # PyYAML\n","except Exception as e:\n","    raise ImportError(\"Missing dependency: PyYAML (import yaml). Please install/enable PyYAML in this Fabric environment.\") from e\n","\n","from pyspark.sql import Row\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import (\n","    StructType, StructField, StringType, TimestampType\n",")\n","\n","RUN_TS_UTC = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S%z\")\n","\n","# Location of the registry YAMLs in your Lakehouse Files\n","# Put your files under: Files/governance/schema_registry/bronze/\n","REGISTRY_DIR = \"Files/governance/schema_registry/bronze\"\n","\n","# Output table (Delta) where compliance results will be appended\n","COMPLIANCE_TABLE = \"tech_schema_compliance\"\n","\n","# Behavior flags\n","STRICT_ORDER_CHECK = True     # if True, order differences => FAIL\n","FAIL_ON_MISSING_TABLE = False # if True, missing table => FAIL; else status = MISSING_TABLE\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c2eae845-be75-4a2f-a9fa-a0b4811d5951"},{"cell_type":"code","source":["def _ls(path: str):\n","    # Fabric notebooks typically provide mssparkutils\n","    try:\n","        return mssparkutils.fs.ls(path)\n","    except Exception:\n","        # fallback to dbutils if available\n","        try:\n","            return dbutils.fs.ls(path)\n","        except Exception as e:\n","            raise RuntimeError(f\"Unable to list files at {path}. Ensure the path exists in Lakehouse Files.\") from e\n","\n","def _read_text(path: str) -> str:\n","    # mssparkutils.fs.head is convenient for small YAML files\n","    try:\n","        return mssparkutils.fs.head(path, 1024 * 1024)  # 1MB\n","    except Exception:\n","        try:\n","            return dbutils.fs.head(path, 1024 * 1024)\n","        except Exception as e:\n","            raise RuntimeError(f\"Unable to read file: {path}\") from e\n","\n","def list_yaml_files(registry_dir: str):\n","    files = _ls(registry_dir)\n","    yaml_paths = []\n","    for f in files:\n","        p = f.path if hasattr(f, \"path\") else f[0]\n","        if p.lower().endswith(\".yaml\") and not p.lower().endswith(\"_template.yaml\") and not p.lower().endswith(\"_registry_index.yaml\"):\n","            yaml_paths.append(p)\n","    return sorted(yaml_paths)\n","\n","yaml_files = list_yaml_files(REGISTRY_DIR)\n","print(f\"Found {len(yaml_files)} registry YAML files:\")\n","for p in yaml_files:\n","    print(\" -\", p)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"40c1b2c2-efc8-4c6a-bcf7-64615dba3683"},{"cell_type":"code","source":["def canonical_type(t: str) -> str:\n","    \"\"\"Normalize types to a common uppercase vocabulary.\"\"\"\n","    if t is None:\n","        return \"UNKNOWN\"\n","    t = t.strip().lower()\n","    mapping = {\n","        \"string\": \"STRING\",\n","        \"varchar\": \"STRING\",\n","        \"char\": \"STRING\",\n","        \"int\": \"INT\",\n","        \"integer\": \"INT\",\n","        \"bigint\": \"BIGINT\",\n","        \"long\": \"BIGINT\",\n","        \"double\": \"DOUBLE\",\n","        \"float\": \"DOUBLE\",\n","        \"decimal\": \"DECIMAL\",\n","        \"timestamp\": \"TIMESTAMP\",\n","        \"date\": \"DATE\",\n","        \"boolean\": \"BOOLEAN\",\n","        \"bool\": \"BOOLEAN\",\n","    }\n","    # Keep parametric types reasonably (e.g. decimal(10,2))\n","    if t.startswith(\"decimal\"):\n","        return \"DECIMAL\"\n","    return mapping.get(t, t.upper())\n","\n","def spark_field_type_to_registry(ftype) -> str:\n","    \"\"\"Convert Spark data type to registry canonical type.\"\"\"\n","    # ftype.simpleString() yields e.g. 'bigint', 'string', 'timestamp', 'date'\n","    return canonical_type(ftype.simpleString())\n","\n","def load_registry_yaml(path: str) -> dict:\n","    raw = _read_text(path)\n","    doc = yaml.safe_load(raw)\n","\n","    # Basic validation\n","    required_keys = [\"entity\", \"table_name\", \"business_columns\", \"technical_columns\"]\n","    for k in required_keys:\n","        if k not in doc:\n","            raise ValueError(f\"Registry file {path} missing required key: {k}\")\n","\n","    # Flatten expected columns in order\n","    expected_cols = []\n","    for col in doc[\"business_columns\"] + doc[\"technical_columns\"]:\n","        expected_cols.append({\n","            \"name\": col[\"name\"],\n","            \"type\": canonical_type(col[\"type\"]),\n","            \"nullable\": bool(col.get(\"nullable\", True))\n","        })\n","\n","    return {\n","        \"registry_path\": path,\n","        \"entity\": doc[\"entity\"],\n","        \"table_name\": doc[\"table_name\"],\n","        \"layer\": doc.get(\"layer\", \"BRONZE\"),\n","        \"status_declared\": doc.get(\"status\", \"UNKNOWN\"),\n","        \"expected_columns\": expected_cols,\n","        \"constraints\": doc.get(\"constraints\", {})\n","    }\n","\n","def get_live_table_schema(table_name: str):\n","    \"\"\"Return live table schema columns in order as list of dicts.\"\"\"\n","    df = spark.table(table_name)\n","    live = []\n","    for f in df.schema.fields:\n","        live.append({\n","            \"name\": f.name,\n","            \"type\": canonical_type(spark_field_type_to_registry(f.dataType)),\n","            \"nullable\": bool(f.nullable)\n","        })\n","    return live\n","\n","def compare_expected_vs_live(expected, live, strict_order=True):\n","    exp_names = [c[\"name\"] for c in expected]\n","    live_names = [c[\"name\"] for c in live]\n","\n","    exp_set = set(exp_names)\n","    live_set = set(live_names)\n","\n","    missing = [c for c in exp_names if c not in live_set]\n","    extra = [c for c in live_names if c not in exp_set]\n","\n","    # Type mismatches on intersection\n","    exp_map = {c[\"name\"]: c[\"type\"] for c in expected}\n","    live_map = {c[\"name\"]: c[\"type\"] for c in live}\n","\n","    type_mismatches = []\n","    for name in sorted(exp_set.intersection(live_set)):\n","        if exp_map[name] != live_map[name]:\n","            type_mismatches.append({\"column\": name, \"expected\": exp_map[name], \"actual\": live_map[name]})\n","\n","    # Order check (only for common columns)\n","    order_mismatch = False\n","    order_details = None\n","    if strict_order:\n","        # Compare full ordered lists (names only)\n","        order_mismatch = (exp_names != live_names)\n","        if order_mismatch:\n","            order_details = {\n","                \"expected_order\": exp_names,\n","                \"actual_order\": live_names\n","            }\n","\n","    # Determine status\n","    if missing or extra or type_mismatches or (strict_order and order_mismatch):\n","        status = \"FAIL\"\n","    else:\n","        status = \"PASS\"\n","\n","    return {\n","        \"status\": status,\n","        \"missing_columns\": missing,\n","        \"extra_columns\": extra,\n","        \"type_mismatches\": type_mismatches,\n","        \"order_mismatch\": order_mismatch,\n","        \"order_details\": order_details\n","    }\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ea2a6f1e-a6d1-4019-9b39-45faab870b72"},{"cell_type":"code","source":["results = []\n","\n","for ypath in yaml_files:\n","    reg = load_registry_yaml(ypath)\n","    entity = reg[\"entity\"]\n","    table_name = reg[\"table_name\"]\n","\n","    # In Fabric, Lakehouse tables are usually addressable without db prefix.\n","    # If you prefer fully qualified: f\"lh_wm_core.{table_name}\"\n","    table_ref = table_name\n","\n","    try:\n","        live_cols = get_live_table_schema(table_ref)\n","        comparison = compare_expected_vs_live(reg[\"expected_columns\"], live_cols, strict_order=STRICT_ORDER_CHECK)\n","\n","        results.append({\n","            \"run_ts_utc\": RUN_TS_UTC,\n","            \"layer\": reg[\"layer\"],\n","            \"entity\": entity,\n","            \"table_name\": table_name,\n","            \"registry_path\": reg[\"registry_path\"],\n","            \"registry_status\": reg[\"status_declared\"],\n","            \"compliance_status\": comparison[\"status\"],\n","            \"missing_columns\": json.dumps(comparison[\"missing_columns\"]),\n","            \"extra_columns\": json.dumps(comparison[\"extra_columns\"]),\n","            \"type_mismatches\": json.dumps(comparison[\"type_mismatches\"]),\n","            \"order_mismatch\": str(comparison[\"order_mismatch\"]),\n","            \"order_details\": json.dumps(comparison[\"order_details\"]) if comparison[\"order_details\"] else None\n","        })\n","\n","    except Exception as e:\n","        # Missing table, permission, etc.\n","        status = \"MISSING_TABLE\" if not FAIL_ON_MISSING_TABLE else \"FAIL\"\n","        results.append({\n","            \"run_ts_utc\": RUN_TS_UTC,\n","            \"layer\": reg[\"layer\"],\n","            \"entity\": entity,\n","            \"table_name\": table_name,\n","            \"registry_path\": reg[\"registry_path\"],\n","            \"registry_status\": reg[\"status_declared\"],\n","            \"compliance_status\": status,\n","            \"missing_columns\": json.dumps([]),\n","            \"extra_columns\": json.dumps([]),\n","            \"type_mismatches\": json.dumps([]),\n","            \"order_mismatch\": \"false\",\n","            \"order_details\": None\n","        })\n","        print(f\"[WARN] {entity} ({table_name}) validation error: {e}\")\n","\n","schema = StructType([\n","    StructField(\"run_ts_utc\", StringType(), False),\n","    StructField(\"layer\", StringType(), False),\n","    StructField(\"entity\", StringType(), False),\n","    StructField(\"table_name\", StringType(), False),\n","    StructField(\"registry_path\", StringType(), False),\n","    StructField(\"registry_status\", StringType(), True),\n","    StructField(\"compliance_status\", StringType(), False),\n","    StructField(\"missing_columns\", StringType(), True),\n","    StructField(\"extra_columns\", StringType(), True),\n","    StructField(\"type_mismatches\", StringType(), True),\n","    StructField(\"order_mismatch\", StringType(), True),\n","    StructField(\"order_details\", StringType(), True),\n","])\n","\n","df_results = spark.createDataFrame([Row(**r) for r in results], schema=schema)\n","\n","display(df_results.orderBy(F.col(\"compliance_status\").desc(), F.col(\"entity\")))\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1263e269-b048-4106-b73e-fd46b63193e1"},{"cell_type":"code","source":["# Create table if not exists (append-only audit log of compliance runs)\n","spark.sql(f\"\"\"\n","CREATE TABLE IF NOT EXISTS {COMPLIANCE_TABLE} (\n","  run_ts_utc STRING,\n","  layer STRING,\n","  entity STRING,\n","  table_name STRING,\n","  registry_path STRING,\n","  registry_status STRING,\n","  compliance_status STRING,\n","  missing_columns STRING,\n","  extra_columns STRING,\n","  type_mismatches STRING,\n","  order_mismatch STRING,\n","  order_details STRING\n",")\n","USING delta\n","\"\"\")\n","\n","df_results.write.mode(\"append\").saveAsTable(COMPLIANCE_TABLE)\n","\n","print(f\"Appended {df_results.count()} rows to {COMPLIANCE_TABLE}.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"464fb09e-e0a7-498b-a7d1-e58cb8c8a408"},{"cell_type":"code","source":["summary = (\n","    df_results\n","    .groupBy(\"compliance_status\")\n","    .count()\n","    .orderBy(F.desc(\"count\"))\n",")\n","\n","display(summary)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"86c6c712-1249-4c4f-96a6-02a136cb8cfc"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}