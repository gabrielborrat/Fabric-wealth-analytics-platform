{"cells":[{"cell_type":"code","source":["# PARAMETER_CELL\n","# Ces paramètres sont alimentés par le pipeline Fabric (Base parameters)\n","\n","entity = \"\"             # ex: \"FX\", \"CUSTOMER\"\n","file_path = \"\"          # ex: \"FX-rates-since2004-dataset/file.csv\"\n","source_name = \"\"        # ex: \"file.csv\"\n","file_size = \"-1\"        # string dans les paramètres, on cast en long\n","modified_datetime = \"\"  # ex: \"2025-11-30T10:15:00Z\" ou vide si non dispo\n","exec_date = \"\"          # ex: \"2025-11-30\"\n","status = \"\"             # \"SUCCESS\", \"FAILED\", \"PARTIAL\", etc.\n","ingestion_mode = \"\"\n","total_source_files = \"0\"\n","total_candidate_files = \"0\"\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"04d61999-35ac-4c00-97d6-5362f629752d"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","from pyspark.sql.utils import AnalysisException\n","from notebookutils import mssparkutils\n","\n","print(\"== nb_update_manifest ==\")\n","print(f\"entity               = {entity}\")\n","print(f\"file_path            = {file_path}\")\n","print(f\"source_name          = {source_name}\")\n","print(f\"file_size (raw)      = {file_size}\")\n","print(f\"modified_datetime    = {modified_datetime}\")\n","print(f\"exec_date            = {exec_date}\")\n","print(f\"status               = {status}\")\n","print(f\"ingestion_mode     = {ingestion_mode}\")\n","print(f\"total_source_files   = {total_source_files}\")\n","print(f\"total_candidate_files= {total_candidate_files}\")\n","\n","# Sécurisation minimale\n","if not entity or not file_path:\n","    print(\"ERREUR : entity ou file_path manquant → aucune mise à jour manifest.\")\n","    mssparkutils.notebook.exit(\"NO_UPDATE\")\n","\n","if not status:\n","    status = \"UNKNOWN\"\n","\n","if not ingestion_mode:\n","    ingestion_mode = \"UNKNOWN\"\n","\n","# Cast file_size en long\n","try:\n","    file_size_long = int(file_size)\n","except Exception:\n","    file_size_long = -1\n","\n","# Cast des compteurs en int\n","try:\n","    total_source_files_int = int(total_source_files)\n","except Exception:\n","    total_source_files_int = -1\n","\n","try:\n","    total_candidate_files_int = int(total_candidate_files)\n","except Exception:\n","    total_candidate_files_int = -1\n","\n","# On garde modified_datetime tel quel, on le cast plus tard si non vide\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7955e71e-c5bc-42c0-bde4-be1665f518e0"},{"cell_type":"code","source":["from datetime import datetime\n","\n","# Construction d'un seul enregistrement pour ce fichier\n","data = [(entity,\n","         file_path,\n","         source_name,\n","         file_size_long,\n","         modified_datetime if modified_datetime else None,\n","         exec_date,\n","         status,\n","         ingestion_mode,\n","         total_source_files_int,\n","         total_candidate_files_int\n","        )]\n","\n","schema = StructType([\n","    StructField(\"entity\", StringType(), False),\n","    StructField(\"file_path\", StringType(), False),\n","    StructField(\"source_name\", StringType(), True),\n","    StructField(\"file_size\", LongType(), True),\n","\n","    StructField(\"modified_datetime_raw\", StringType(), True),\n","    StructField(\"exec_date_raw\", StringType(), True),\n","    StructField(\"last_status\", StringType(), True),\n","\n","    StructField(\"ingestion_mode\", StringType(), True),\n","    StructField(\"total_source_files_raw\", IntegerType(), True),\n","    StructField(\"total_candidate_files_raw\", IntegerType(), True),\n","])\n","\n","df_stage = spark.createDataFrame(data, schema)\n","\n","# Casts propres + normalisation des colonnes\n","df_stage = (\n","    df_stage\n","    .withColumn(\n","        \"modified_datetime\",\n","        F.when(\n","            F.col(\"modified_datetime_raw\").isNotNull() & (F.col(\"modified_datetime_raw\") != \"\"),\n","            F.to_timestamp(\"modified_datetime_raw\")\n","        ).otherwise(F.lit(None).cast(\"timestamp\"))\n","    )\n","    .withColumn(\n","        \"exec_date\",\n","        F.to_date(\"exec_date_raw\")\n","    )\n","    .withColumn(\"first_ingested_datetime\", F.current_timestamp())\n","    .withColumn(\"last_ingested_datetime\", F.current_timestamp())\n","    .withColumnRenamed(\"total_source_files_raw\", \"total_source_files\")\n","    .withColumnRenamed(\"total_candidate_files_raw\", \"total_candidate_files\")\n","    .select(\n","        \"entity\",\n","        \"file_path\",\n","        \"source_name\",\n","        \"file_size\",\n","        \"modified_datetime\",\n","        \"first_ingested_datetime\",\n","        \"last_ingested_datetime\",\n","        \"last_status\",\n","        \"exec_date\",\n","        \"ingestion_mode\",\n","        \"total_source_files\",\n","        \"total_candidate_files\"\n","    )\n",")\n","\n","print(\"Schéma staging (df_stage) :\")\n","df_stage.printSchema()\n","display(df_stage)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c8967cb8-fb18-4477-a022-f54cb6a84ba3"},{"cell_type":"code","source":["# Vérification / création de la table tech_ingestion_manifest\n","try:\n","    spark.table(\"tech_ingestion_manifest\")\n","    print(\"Table tech_ingestion_manifest déjà existante.\")\n","except AnalysisException:\n","    print(\"Table tech_ingestion_manifest absente, création en cours...\")\n","\n","    (\n","        df_stage.limit(0)   # Ne crée que le schéma, pas de données\n","        .write\n","        .format(\"delta\")\n","        .mode(\"overwrite\")\n","        .saveAsTable(\"tech_ingestion_manifest\")\n","    )\n","\n","    print(\"Table tech_ingestion_manifest créée avec le schéma complet.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"39bdf79d-b7cd-4b25-8d53-5f32f340dd3c"},{"cell_type":"code","source":["# Création de la vue staging\n","df_stage.createOrReplaceTempView(\"stg_manifest_update\")\n","\n","merge_sql = \"\"\"\n","MERGE INTO tech_ingestion_manifest AS tgt\n","USING stg_manifest_update AS src\n","  ON tgt.entity = src.entity\n"," AND tgt.file_path = src.file_path\n","\n","WHEN MATCHED THEN\n","  UPDATE SET\n","    tgt.last_status             = src.last_status,\n","    tgt.last_ingested_datetime  = src.last_ingested_datetime,\n","    tgt.exec_date               = src.exec_date,\n","    tgt.source_name             = src.source_name,\n","    tgt.file_size               = src.file_size,\n","    tgt.modified_datetime       = src.modified_datetime,\n","    tgt.ingestion_mode        = src.ingestion_mode,\n","    tgt.total_source_files      = src.total_source_files,\n","    tgt.total_candidate_files   = src.total_candidate_files\n","\n","WHEN NOT MATCHED THEN\n","  INSERT (\n","    entity,\n","    file_path,\n","    source_name,\n","    file_size,\n","    modified_datetime,\n","    first_ingested_datetime,\n","    last_ingested_datetime,\n","    last_status,\n","    exec_date,\n","    ingestion_mode,\n","    total_source_files,\n","    total_candidate_files\n","  )\n","  VALUES (\n","    src.entity,\n","    src.file_path,\n","    src.source_name,\n","    src.file_size,\n","    src.modified_datetime,\n","    src.first_ingested_datetime,\n","    src.last_ingested_datetime,\n","    src.last_status,\n","    src.exec_date,\n","    src.ingestion_mode,\n","    src.total_source_files,\n","    src.total_candidate_files\n","  );\n","\"\"\"\n","\n","print(\"Exécution du MERGE dans tech_ingestion_manifest...\")\n","spark.sql(merge_sql)\n","print(\"MERGE terminé.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"66dcc8dc-e322-4bcb-bcfc-8a2f5f96e30c"},{"cell_type":"code","source":["# ============================================================\n","# Cellule 6 — Retour structuré au pipeline\n","# ------------------------------------------------------------\n","# Cette cellule n’est pas obligatoire pour le fonctionnement\n","# du pipeline, mais elle est fortement recommandée :\n","#\n","# ✔ Permet de visualiser, dans l'Output de l’activité Notebook,\n","#   les informations clés sur le fichier traité.\n","# ✔ Facilite énormément le debug lors du développement,\n","#   du run initial et des tests incrémentaux.\n","# ✔ Permet au pipeline parent (si Invoke Pipeline) d’exploiter\n","#   les valeurs renvoyées (optionnel).\n","#\n","# Le résultat est sérialisé en JSON via mssparkutils.notebook.exit\n","# (API Fabric-compatible), puis stocké dans les logs du pipeline.\n","# ============================================================\n","\n","import json\n","\n","result = {\n","    \"entity\": entity,\n","    \"file_path\": file_path,\n","    \"source_name\": source_name,\n","    \"status\": status,\n","    \"ingestion_mode\": ingestion_mode,\n","    \"total_source_files\": total_source_files_int,\n","    \"total_candidate_files\": total_candidate_files_int,\n","    \"last_ingested_timestamp\": str(datetime.now())\n","}\n","\n","print(\"== Résultat renvoyé au pipeline ==\")\n","print(json.dumps(result, indent=2))\n","\n","# Envoi au pipeline (visible dans l’Output de l’activité Notebook)\n","mssparkutils.notebook.exit(json.dumps(result))\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5e9beaaa-f836-4d2c-bb7f-c746ed1c17ae"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}