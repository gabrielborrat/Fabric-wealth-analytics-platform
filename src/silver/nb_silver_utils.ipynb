{"cells":[{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","from delta.tables import DeltaTable\n","\n","SILVER_DB = \"\"   # si vous utilisez le default Lakehouse\n","\n","# ------------------------------------------------------------\n","# TRANSACTIONS conventions\n","# ------------------------------------------------------------\n","\n","TRANSACTIONS_PARTITION_COLS = [\"txn_month\"]\n","\n","\n","def add_txn_dates(df, ts_col=\"txn_ts\"):\n","    if ts_col not in df.columns:\n","        raise ValueError(f\"Column '{ts_col}' not found in dataframe\")\n","    return (\n","        df\n","        .withColumn(\"txn_date\", F.to_date(F.col(ts_col)))\n","        .withColumn(\"txn_month\", F.date_trunc(\"month\", F.col(ts_col)).cast(\"date\"))\n","    )\n","\n","\n","def normalize_mcc(df, col=\"mcc\"):\n","    return df.withColumn(\n","        \"mcc_code\",\n","        F.lpad(F.col(col).cast(\"string\"), 4, \"0\")\n","    )\n","\n","def cast_amount(df, col=\"amount\", precision=18, scale=2):\n","    return df.withColumn(\n","        col,\n","        F.col(col).cast(f\"decimal({precision},{scale})\")\n","    )\n","\n","def add_tech_columns(\n","    df,\n","    source_file_col=\"source_file\",\n","    ingestion_ts_col=\"ingestion_ts\",\n","    ingestion_date_col=\"ingestion_date\",\n","    default_source_file=None\n","):\n","    \n","    # Standardise les colonnes techniques Silver.\n","    # - source_file : renommée si nécessaire\n","    # - ingestion_ts / ingestion_date : conservées si présentes, sinon générées\n","    \n","\n","    # --- source_file ---\n","    if source_file_col in df.columns and source_file_col != \"source_file\":\n","        df = df.withColumnRenamed(source_file_col, \"source_file\")\n","    elif \"source_file\" not in df.columns:\n","        df = df.withColumn(\"source_file\", F.lit(default_source_file))\n","\n","    # --- ingestion_ts ---\n","    if ingestion_ts_col in df.columns:\n","        if ingestion_ts_col != \"ingestion_ts\":\n","            df = df.withColumnRenamed(ingestion_ts_col, \"ingestion_ts\")\n","    else:\n","        df = df.withColumn(\"ingestion_ts\", F.current_timestamp())\n","\n","    # --- ingestion_date ---\n","    if ingestion_date_col in df.columns:\n","        if ingestion_date_col != \"ingestion_date\":\n","            df = df.withColumnRenamed(ingestion_date_col, \"ingestion_date\")\n","    else:\n","        df = df.withColumn(\"ingestion_date\", F.to_date(F.col(\"ingestion_ts\")))\n","\n","    return df\n","\n","\n","def add_record_hash(df, cols):\n","    missing = [c for c in cols if c not in df.columns]\n","    if missing:\n","        raise ValueError(f\"Missing columns for record_hash: {missing}\")\n","\n","    return df.withColumn(\n","        \"record_hash\",\n","        F.sha2(\n","            F.concat_ws(\n","                \"||\",\n","                *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"∅\")) for c in cols]\n","            ),\n","            256\n","        )\n","    )\n","\n","def assert_required_columns(df, cols):\n","    missing = [c for c in cols if c not in df.columns]\n","    if missing:\n","        raise ValueError(f\"Missing required columns: {missing}\")\n","\n","\n","def deduplicate_latest(\n","    df,\n","    key_cols,\n","    order_col=\"ingestion_ts\"\n","):\n","    w = Window.partitionBy(*key_cols).orderBy(F.col(order_col).desc())\n","    return (\n","        df\n","        .withColumn(\"rn\", F.row_number().over(w))\n","        .filter(\"rn = 1\")\n","        .drop(\"rn\")\n","    )\n","\n","def validate_partitions(\n","    df, \n","    partition_cols, \n","    expect_date_types=True, \n","    allow_nulls=False, \n","    max_distinct_threshold=None):\n","\n","    if partition_cols is None or len(partition_cols) == 0:\n","        return True\n","\n","    #   Valide les colonnes de partition avant écriture Delta.\n","\n","    #   Paramètres\n","    #   ----------\n","    #   df : DataFrame\n","    #   partition_cols : list[str]\n","    #       Colonnes utilisées pour le partitionnement (ex: [\"txn_month\"])\n","    #   expect_date_types : bool\n","    #       Si True, impose DateType pour les colonnes de partition\n","    #   allow_nulls : bool\n","    #       Si False, interdit les NULL dans les partitions\n","    #   max_distinct_threshold : int | None\n","    #       Si défini, alerte si le nombre de partitions distinctes dépasse ce seuil\n","    #\n","    #    Raises\n","    #    ------\n","    #   ValueError en cas de violation bloquante\n","    \n","\n","    # 1) Existence des colonnes\n","    missing = [c for c in partition_cols if c not in df.columns]\n","    if missing:\n","        raise ValueError(f\"Missing partition columns: {missing}\")\n","\n","    # 2) Types\n","    if expect_date_types:\n","        bad_types = []\n","        for c in partition_cols:\n","            dtype = dict(df.dtypes).get(c)\n","            if dtype != \"date\":\n","                bad_types.append((c, dtype))\n","        if bad_types:\n","            raise ValueError(\n","                \"Partition columns must be of type 'date'. Invalid: \"\n","                + \", \".join([f\"{c}({t})\" for c, t in bad_types])\n","            )\n","\n","    # 3) Nullabilité\n","    if not allow_nulls:\n","        null_checks = [\n","            F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n","            for c in partition_cols\n","        ]\n","        nulls = df.select(*null_checks).collect()[0].asDict()\n","        offenders = {c: v for c, v in nulls.items() if v > 0}\n","        if offenders:\n","            raise ValueError(\n","                f\"Null values found in partition columns: {offenders}\"\n","            )\n","\n","    # 4) Cardinalité (anti-explosion de partitions)\n","    if max_distinct_threshold is not None:\n","        distinct_count = (\n","            df.select(*partition_cols).distinct().count()\n","        )\n","        if distinct_count > max_distinct_threshold:\n","            raise ValueError(\n","                f\"Partition cardinality too high ({distinct_count}) \"\n","                f\"> threshold ({max_distinct_threshold}).\"\n","            )\n","\n","    return True\n","\n","\n","\n","def write_silver_delta(df, table_name, partition_cols=None, mode=\"overwrite\"):\n","    writer = (\n","        df.write\n","        .format(\"delta\")\n","        .mode(mode)\n","        .option(\"overwriteSchema\", \"true\")\n","    )\n","\n","    if partition_cols and len(partition_cols) > 0:\n","        writer = writer.partitionBy(*partition_cols)\n","\n","    writer.saveAsTable(table_name)\n","\n","\n","\n","def write_silver_transactions(\n","    df,\n","    table_name=\"silver_transactions\",\n","    mode=\"overwrite\"\n","):\n","    validate_partitions(\n","        df,\n","        partition_cols=TRANSACTIONS_PARTITION_COLS,\n","        expect_date_types=True,\n","        allow_nulls=False,\n","        max_distinct_threshold=240\n","    )\n","\n","    write_silver_delta(\n","        df,\n","        table_name=table_name,\n","        partition_cols=TRANSACTIONS_PARTITION_COLS,\n","        mode=mode\n","    )\n","\n","\n","def merge_silver_delta(df, table_name, key_cols):\n","    if not spark.catalog.tableExists(table_name):\n","        df.write.format(\"delta\").saveAsTable(table_name)\n","        return\n","\n","    tgt = DeltaTable.forName(spark, table_name)\n","    cond = \" AND \".join([f\"t.{k} = s.{k}\" for k in key_cols])\n","\n","    (\n","        tgt.alias(\"t\")\n","        .merge(df.alias(\"s\"), cond)\n","        .whenMatchedUpdateAll()\n","        .whenNotMatchedInsertAll()\n","        .execute()\n","    )\n","\n","# ------------------------------------------------------------\n","# FX conventions\n","# ------------------------------------------------------------\n","FX_PARTITION_COLS = [\"fx_month\"]\n","\n","def add_fx_dates(df, date_col=\"fx_date\"):\n","    if date_col not in df.columns:\n","        raise ValueError(f\"Column '{date_col}' not found in dataframe\")\n","    return (\n","        df\n","        .withColumn(\"fx_date\", F.to_date(F.col(date_col)))\n","        .withColumn(\"fx_month\", F.date_trunc(\"month\", F.col(\"fx_date\")).cast(\"date\"))\n","    )\n","\n","def normalize_currency_codes(df, cols):\n","    for c in cols:\n","        if c not in df.columns:\n","            raise ValueError(f\"Column '{c}' not found in dataframe\")\n","        df = df.withColumn(c, F.upper(F.trim(F.col(c))))\n","    return df\n","\n","def cast_rate(df, col=\"rate\", precision=18, scale=8):\n","    if col not in df.columns:\n","        raise ValueError(f\"Column '{col}' not found in dataframe\")\n","    return df.withColumn(col, F.col(col).cast(f\"decimal({precision},{scale})\"))\n","\n","\n","def write_silver_fx_rates(\n","    df,\n","    table_name=\"silver_fx_rates\",\n","    mode=\"overwrite\"\n","):\n","    validate_partitions(\n","        df,\n","        partition_cols=FX_PARTITION_COLS,\n","        expect_date_types=True,\n","        allow_nulls=False,\n","        max_distinct_threshold=240\n","    )\n","\n","    write_silver_delta(\n","        df,\n","        table_name=table_name,\n","        partition_cols=FX_PARTITION_COLS,\n","        mode=mode\n","    )\n","\n","# ------------------------------------------------------------\n","# MCC, USER, CARD conventions\n","# ------------------------------------------------------------\n","\n","def normalize_text(df, cols):\n","    for c in cols:\n","        if c in df.columns:\n","            df = df.withColumn(c, F.upper(F.trim(F.col(c))))\n","    return df\n","\n","def cast_decimal(df, col, precision=18, scale=2):\n","    if col not in df.columns:\n","        raise ValueError(f\"Column '{col}' not found in dataframe\")\n","    return df.withColumn(col, F.col(col).cast(f\"decimal({precision},{scale})\"))\n","\n","def normalize_mcc_code(df, col=\"mcc\"):\n","    if col not in df.columns:\n","        raise ValueError(f\"Column '{col}' not found in dataframe\")\n","    return df.withColumn(\"mcc_code\", F.lpad(F.col(col).cast(\"string\"), 4, \"0\"))\n","\n","def parse_bool_yn(df, col, true_values=(\"Y\",\"YES\",\"TRUE\",\"1\"), false_values=(\"N\",\"NO\",\"FALSE\",\"0\")):\n","    if col not in df.columns:\n","        raise ValueError(f\"Column '{col}' not found in dataframe\")\n","    c = F.upper(F.trim(F.col(col).cast(\"string\")))\n","    return df.withColumn(\n","        col,\n","        F.when(c.isin([v.upper() for v in true_values]), F.lit(True))\n","         .when(c.isin([v.upper() for v in false_values]), F.lit(False))\n","         .otherwise(F.lit(None).cast(\"boolean\"))\n","    )\n","\n","def parse_date_multi(df, col, formats):\n","    if col not in df.columns:\n","        raise ValueError(f\"Column '{col}' not found in dataframe\")\n","    parsed = None\n","    for fmt in formats:\n","        candidate = F.to_date(F.col(col).cast(\"string\"), fmt)\n","        parsed = candidate if parsed is None else F.coalesce(parsed, candidate)\n","    return df.withColumn(col, parsed)\n","\n","def write_silver_mcc(df, table_name=\"silver_mcc\", mode=\"overwrite\"):\n","    write_silver_delta(df, table_name=table_name, partition_cols=[], mode=mode)\n","\n","def write_silver_users(df, table_name=\"silver_users\", mode=\"overwrite\"):\n","    write_silver_delta(df, table_name=table_name, partition_cols=[], mode=mode)\n","\n","def write_silver_cards(df, table_name=\"silver_cards\", mode=\"overwrite\"):\n","    write_silver_delta(df, table_name=table_name, partition_cols=[], mode=mode)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e72f9371-191b-48f1-b83e-d11e156dabb3"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}