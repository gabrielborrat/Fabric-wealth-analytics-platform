{"cells":[{"cell_type":"code","source":["# ============================================================\n","# nb_silver_22_cards\n","# ============================================================\n","\n","# %run ./nb_silver_utils\n","from pyspark.sql import functions as F\n","\n","df_bronze = spark.table(\"bronze_card_raw\")\n","\n","assert_required_columns(\n","    df_bronze,\n","    [\"id\", \"client_id\", \"card_brand\", \"card_type\", \"card_number\",\n","     \"expires\", \"cvv\", \"has_chip\", \"num_cards_issued\", \"credit_limit\",\n","     \"acct_open_date\", \"year_pin_last_changed\", \"card_on_dark_web\",\n","     \"source_file\", \"ingestion_ts\", \"ingestion_date\"]\n",")\n","\n","# Renaming\n","df = df_bronze.withColumnRenamed(\"id\", \"card_id\")\n","\n","# Normalisation texte\n","df = normalize_text(df, cols=[\"card_brand\", \"card_type\"])\n","\n","# Boolean parsing (has_chip / card_on_dark_web viennent en STRING) :contentReference[oaicite:13]{index=13}\n","df = parse_bool_yn(df, \"has_chip\")\n","df = parse_bool_yn(df, \"card_on_dark_web\")\n","\n","# credit_limit -> DECIMAL(18,2)\n","df = cast_decimal(df, \"credit_limit\", precision=18, scale=2)\n","\n","# Dates : acct_open_date (string) => DATE\n","df = parse_date_multi(df, \"acct_open_date\", formats=[\"yyyy-MM-dd\", \"MM/dd/yyyy\", \"M/d/yyyy\"])\n","\n","# expires : garder aussi en string, mais on crée une date normalisée (optionnelle)\n","# Hypothèse fréquente: \"MM/YY\"\n","df = df.withColumn(\"expires_raw\", F.col(\"expires\").cast(\"string\"))\n","df = df.withColumn(\n","    \"expires_month\",\n","    F.to_date(F.concat(F.lit(\"01/\"), F.col(\"expires_raw\")), \"dd/MM/yy\")\n",")\n","\n","# Colonnes techniques\n","df = add_tech_columns(df, source_file_col=\"source_file\")\n","\n","# Hash\n","hash_cols = [\n","    \"card_id\", \"client_id\", \"card_brand\", \"card_type\", \"card_number\",\n","    \"expires_raw\", \"expires_month\", \"cvv\", \"has_chip\", \"num_cards_issued\",\n","    \"credit_limit\", \"acct_open_date\", \"year_pin_last_changed\", \"card_on_dark_web\"\n","]\n","df = add_record_hash(df, cols=[c for c in hash_cols if c in df.columns])\n","\n","# Dédup (clé naturelle = id -> card_id) :contentReference[oaicite:14]{index=14}\n","df = deduplicate_latest(df, key_cols=[\"card_id\"], order_col=\"ingestion_ts\")\n","\n","# Projection Silver\n","df = df.select(\n","    \"card_id\",\n","    \"client_id\",\n","    \"card_brand\",\n","    \"card_type\",\n","    \"card_number\",\n","    \"expires_raw\",\n","    \"expires_month\",\n","    \"cvv\",\n","    \"has_chip\",\n","    \"num_cards_issued\",\n","    \"credit_limit\",\n","    \"acct_open_date\",\n","    \"year_pin_last_changed\",\n","    \"card_on_dark_web\",\n","    \"source_file\",\n","    \"ingestion_date\",\n","    \"ingestion_ts\",\n","    \"record_hash\"\n",")\n","\n","# Fail fast : clé non nulle\n","if df.filter(F.col(\"card_id\").isNull()).limit(1).count() > 0:\n","    raise ValueError(\"Null card_id detected in silver_cards.\")\n","\n","# Fail fast supplémentaire : client_id requis pour la jointure\n","if df.filter(F.col(\"client_id\").isNull()).limit(1).count() > 0:\n","    raise ValueError(\"Null client_id detected in silver_cards (required for joins with users/transactions).\")\n","\n","write_silver_cards(df, table_name=\"silver_cards\", mode=\"overwrite\")\n","print(\"silver_cards successfully written.\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5e5e0ad8-5e40-4469-bd66-c811eac4905a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}