{"cells":[{"cell_type":"code","source":["# ==========================================\n","# PARAMETER CELL (Fabric Notebook)\n","# ==========================================\n","# Ces variables seront surchargées par le pipeline.\n","# valeurs par défaut pour les tests locaux.\n","#\n","# run_id:        string (GUID generated by pipeline master)\n","# entity_code:   string (transactions|fx_rates|mcc|users|cards)\n","# load_mode:     string (full|incremental) - optional (may be empty)\n","# as_of_date:    string (YYYY-MM-DD) - optional (may be empty)\n","# environment:   string (dev|prod)\n","# pipeline_name: string (pl_silver_load_master)\n","#\n","# Example local defaults (ONLY for ad-hoc interactive runs; remove in prod):\n","from datetime import datetime, timezone\n","try:\n","    run_id\n","except NameError:\n","    run_id = f\"manual-{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}\"\n","    entity_code = \"fx_rates\"\n","    load_mode = \"full\"          # will fallback to ctl default\n","    as_of_date = \"\"         # optional\n","    environment = \"dev\"\n","    pipeline_name = \"manual\"\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"88a75acd-3d24-4d49-a3fc-2fe2deb1fe0b","normalized_state":"finished","queued_time":"2025-12-19T14:28:55.2585522Z","session_start_time":null,"execution_start_time":"2025-12-19T14:28:55.2596824Z","execution_finish_time":"2025-12-19T14:28:55.5532683Z","parent_msg_id":"1cef3e46-0ea1-4d40-8e6a-87a3e2849a9d"},"text/plain":"StatementMeta(, 88a75acd-3d24-4d49-a3fc-2fe2deb1fe0b, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"fd7a66a0-0349-4bcc-933d-c97f1a49587b"},{"cell_type":"code","source":["# ============================================================\n","# nb_load_silver  (Microsoft Fabric / Spark Notebook)\n","# Router + logging wrapper for Silver entity notebooks\n","#\n","# IMPORTANT (Fabric parameters):\n","# - In Fabric, define parameters in the first cell using:\n","#     run_id, entity_code, load_mode, as_of_date, environment, pipeline_name\n","# - Then reference them as regular Python variables in subsequent cells.\n","# ============================================================\n","\n","import json\n","import time\n","from datetime import datetime, timezone\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import (\n","    StructType, StructField,\n","    StringType, TimestampType,\n","    LongType, IntegerType\n",")\n","import notebookutils\n","import traceback\n","\n","# Notebook exit exception can come from different namespaces in Fabric.\n","# We keep a best-effort import + a name-based fallback.\n","try:\n","    from notebookutils.mssparkutils.handlers.notebookHandler import NotebookExit  # type: ignore\n","except Exception:\n","    NotebookExit = None  # fallback to name-based detection\n","\n","def _is_notebook_exit(exc: BaseException) -> bool:\n","    return exc.__class__.__name__ == \"NotebookExit\"\n","\n","# Runtime contract location (Lakehouse Files)\n","RUNTIME_CONTRACT_PATH = \"Files/governance/runtime/silver/entity_payload.json\"\n","\n","# ------------------------------------------------------------\n","# Tables de contrôle & logging (alignées sur nb_silver_ddl.py)\n","# ------------------------------------------------------------\n","CTL_TABLE  = \"silver_ctl_entity\"\n","STEP_TABLE = \"silver_log_steps\"\n","\n","# Notes:\n","# - STEP_TABLE contient payload_json (contrat runtime v1.0)\n","# - CTL_TABLE pilote notebook_path, timeout, critical, etc.\n","\n","# ----------------------------\n","# Helpers\n","# ----------------------------\n","def utc_now():\n","    return datetime.now(timezone.utc)\n","\n","def to_int(x, default=None):\n","    try:\n","        return int(x)\n","    except Exception:\n","        return default\n","\n","def safe_str(x, max_len=4000):\n","    s = \"\" if x is None else str(x)\n","    return s[:max_len]\n","\n","def _read_text(path: str) -> str:\n","    \n","    #   Read a small text/JSON file from Lakehouse Files via Fabric utilities.\n","    #   mssparkutils.fs.open() is not available in some Fabric runtimes.\n","    #   Use fs.head() instead (sufficient for small contracts).\n","    \n","    try:\n","    # Prefer notebookutils.fs.head (newer namespace), fallback to mssparkutils.fs.head\n","        try:\n","            return notebookutils.fs.head(path, 1024 * 1024)  # up to 1MB\n","        except Exception:\n","            return mssparkutils.fs.head(path, 1024 * 1024)\n","    except Exception as e:\n","        raise FileNotFoundError(f\"Cannot read runtime contract at '{path}': {e}\")\n","\n","def load_runtime_contract(path: str = RUNTIME_CONTRACT_PATH) -> dict:\n","    \"\"\"\n","    Load runtime payload contract JSON from Lakehouse Files.\n","    \"\"\"\n","    raw = _read_text(path)\n","    try:\n","        return json.loads(raw)\n","    except Exception as e:\n","        raise ValueError(f\"Runtime contract file is not valid JSON: {path}. Error: {e}\")\n","\n","def _extract_dot(obj: dict, dot_path: str):\n","    \"\"\"\n","    Extract nested value using dot notation (e.g., 'metrics.row_out').\n","    Returns None if any segment is missing.\n","    \"\"\"\n","    cur = obj\n","    for seg in dot_path.split(\".\"):\n","        if cur is None:\n","            return None\n","        if isinstance(cur, dict) and seg in cur:\n","            cur = cur.get(seg)\n","        else:\n","            return None\n","    return cur\n","\n","def validate_payload_against_contract(payload: dict, contract: dict):\n","    \"\"\"\n","    Enforce required fields defined in the runtime contract.\n","    The contract is expected to have 'required_fields' as a list of dot-paths.\n","    \"\"\"\n","    required = contract.get(\"required_fields\", [])\n","    if not isinstance(required, list) or len(required) == 0:\n","        raise ValueError(\"Runtime contract is missing a non-empty 'required_fields' list\")\n","\n","    missing = []\n","    for p in required:\n","        v = _extract_dot(payload, p) if isinstance(p, str) else None\n","        if v is None:\n","            missing.append(p)\n","\n","    if missing:\n","        raise ValueError(f\"Entity payload missing required fields: {missing}\")\n","\n","def normalize_entity_payload(child_payload: dict) -> dict:\n","    \"\"\"\n","    Backward compatibility:\n","    - If child notebook returns legacy metrics at root (row_out, etc.), wrap them.\n","    - If it already follows the v1.0 contract, return as-is.\n","    \"\"\"\n","    if isinstance(child_payload, dict) and \"metrics\" in child_payload and \"table\" in child_payload:\n","        return child_payload\n","\n","    # Legacy -> contract-like envelope\n","    now = utc_now()\n","    return {\n","        \"contract_version\": \"0.x-legacy\",\n","        \"layer\": \"silver\",\n","        \"run_id\": child_payload.get(\"run_id\", run_id),\n","        \"entity_code\": child_payload.get(\"entity_code\", entity_code),\n","        \"load_mode\": child_payload.get(\"load_mode\", None),\n","        \"as_of_date\": child_payload.get(\"as_of_date\", None),\n","        \"status\": child_payload.get(\"status\", \"SUCCESS\"),\n","        \"metrics\": {\n","            \"row_in\": child_payload.get(\"row_in\"),\n","            \"row_out\": child_payload.get(\"row_out\"),\n","            \"partition_count\": child_payload.get(\"partition_count\", 0),\n","            \"dedup_dropped\": child_payload.get(\"dedup_dropped\", 0),\n","        },\n","        \"table\": {\n","            \"target_table\": child_payload.get(\"target_table\", None),\n","            \"partition_cols\": child_payload.get(\"partition_cols\", []),\n","        },\n","        \"timing\": {\n","            \"started_utc\": now.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n","            \"ended_utc\": now.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n","            \"duration_ms\": 0,\n","        }\n","    }\n","\n","\n","LOG_STEP_SCHEMA = StructType([\n","    StructField(\"run_id\",          StringType(),   False),\n","    StructField(\"entity_code\",     StringType(),   False),\n","    StructField(\"notebook_name\",   StringType(),   True),\n","    StructField(\"start_ts\",        TimestampType(),True),\n","    StructField(\"end_ts\",          TimestampType(),True),\n","    StructField(\"status\",          StringType(),   False),\n","    StructField(\"row_in\",          LongType(),     True),\n","    StructField(\"row_out\",         LongType(),     True),\n","    StructField(\"partition_count\", IntegerType(),  True),\n","    StructField(\"dedup_dropped\",   LongType(),     True),\n","    StructField(\"error_message\",   StringType(),   True),\n","    StructField(\"payload_json\",    StringType(),   True),\n","])\n","\n","\n","def append_log_step(payload: dict):\n","    # Enforce schema explicitly (avoid Spark inference issues when values are None)\n","    row = (\n","        str(payload.get(\"run_id\")),\n","        str(payload.get(\"entity_code\")),\n","        payload.get(\"notebook_name\"),\n","        payload.get(\"start_ts\"),\n","        payload.get(\"end_ts\"),\n","        str(payload.get(\"status\")),\n","        payload.get(\"row_in\"),\n","        payload.get(\"row_out\"),\n","        payload.get(\"partition_count\"),\n","        payload.get(\"dedup_dropped\"),\n","        payload.get(\"error_message\"),\n","        payload.get(\"payload_json\"),\n","    )\n","    spark.createDataFrame([row], schema=LOG_STEP_SCHEMA) \\\n","         .write.format(\"delta\") \\\n","         .mode(\"append\") \\\n","         .saveAsTable(STEP_TABLE)\n","\n","\n","def exit_notebook(obj: dict):\n","    # Fabric supports mssparkutils.notebook.exit\n","    mssparkutils.notebook.exit(json.dumps(obj))\n","\n","def debug_step(msg):\n","    print(f\"[DEBUG nb_load_silver] {msg}\")\n","\n","# ----------------------------\n","# Start\n","# ----------------------------\n","step_start_ts = utc_now()\n","t0 = time.time()\n","\n","# ------------------------------------------------------------\n","# Resolve entity_code selection (single entity vs ALL/*)\n","# ------------------------------------------------------------\n","entity_code_norm = (entity_code or \"\").strip()\n","entity_code_up = entity_code_norm.upper()\n","\n","is_all = entity_code_up in (\"ALL\", \"*\", \"\")\n","\n","ctl_base = spark.table(CTL_TABLE)\n","\n","if is_all:\n","    # Run all enabled entities ordered by execution_order (if exists)\n","    # (If execution_order column does not exist, fallback to entity_code ordering)\n","    cols = [c.lower() for c in ctl_base.columns]\n","    ctl_enabled = ctl_base.where(F.col(\"enabled\") == F.lit(True))\n","\n","    if \"execution_order\" in cols:\n","        ctl_selected = ctl_enabled.orderBy(F.col(\"execution_order\").asc())\n","    else:\n","        ctl_selected = ctl_enabled.orderBy(F.col(\"entity_code\").asc())\n","\n","    ctl_rows = ctl_selected.collect()\n","    if not ctl_rows:\n","        raise ValueError(f\"No enabled entities found in {CTL_TABLE}.\")\n","else:\n","    ctl_selected = (\n","        ctl_base\n","        .where(F.upper(F.col(\"entity_code\")) == F.lit(entity_code_up))\n","        .limit(1)\n","    )\n","    ctl_rows = ctl_selected.collect()\n","    if not ctl_rows:\n","        raise ValueError(f\"Entity not found in {CTL_TABLE}: {entity_code}\")\n","\n","# Convert to list of configs (one per entity)\n","cfg_list = [r.asDict() for r in ctl_rows]\n","\n","results = []\n","any_failed = False\n","any_failed_critical = False\n","\n","for cfg in cfg_list:\n","    # Each iteration runs exactly the same logic as before,\n","    # but for one entity at a time.\n","    entity_code_current = cfg.get(\"entity_code\")\n","\n","    # Reset per-entity timers\n","    step_start_ts = utc_now()\n","    t0 = time.time()\n","\n","    # Skip if disabled (should not happen in ALL since we filtered enabled=true, but keep it safe)\n","    if not bool(cfg.get(\"enabled\", True)):\n","        append_log_step({\n","            \"run_id\": run_id,\n","            \"entity_code\": entity_code_current,\n","            \"notebook_name\": cfg.get(\"notebook_name\"),\n","            \"start_ts\": step_start_ts,\n","            \"end_ts\": utc_now(),\n","            \"status\": \"SKIPPED\",\n","            \"payload_json\": json.dumps({\"status\": \"SKIPPED\", \"reason\": \"disabled\"})\n","        })\n","        results.append({\"entity_code\": entity_code_current, \"status\": \"SKIPPED\"})\n","        continue\n","\n","    # Resolve load_mode\n","    resolved_load_mode = (load_mode or \"\").strip() or (cfg.get(\"load_mode_default\") or \"full\")\n","\n","    # Log RUNNING\n","    append_log_step({\n","        \"run_id\": run_id,\n","        \"entity_code\": entity_code_current,\n","        \"notebook_name\": cfg.get(\"notebook_name\"),\n","        \"start_ts\": step_start_ts,\n","        \"status\": \"RUNNING\",\n","        \"payload_json\": json.dumps({\n","            \"status\": \"RUNNING\",\n","            \"load_mode\": resolved_load_mode,\n","            \"as_of_date\": as_of_date,\n","            \"environment\": environment,\n","            \"pipeline_name\": pipeline_name\n","        })\n","    })\n","\n","    # Execute entity notebook\n","    try:\n","        nb_path = cfg.get(\"notebook_path\")\n","        if not nb_path:\n","            raise ValueError(f\"Missing notebook_path for entity {entity_code_current} in {CTL_TABLE}\")\n","\n","        timeout_min = to_int(cfg.get(\"timeout_minutes\"), default=60)\n","        timeout_sec = timeout_min * 60\n","        \n","        # Parameters passed down to entity notebook\n","        child_params = {\n","            \"run_id\": run_id,\n","            \"entity_code\": entity_code_current,\n","            \"load_mode\": resolved_load_mode,\n","            \"as_of_date\": as_of_date,\n","            \"environment\": environment\n","        }\n","\n","        # Expect child notebook to exit with a JSON string (entity payload)\n","        child_res = mssparkutils.notebook.run(nb_path, timeout_sec, child_params)\n","        child_obj = json.loads(child_res) if child_res else {}\n","\n","        # Load & apply runtime contract (Files/governance/runtime/silver/entity_payload.json)\n","        contract = load_runtime_contract(RUNTIME_CONTRACT_PATH)\n","\n","        # Normalize (supports legacy payloads during transition)\n","        payload = normalize_entity_payload(child_obj)\n","\n","        # Validate required fields per contract\n","        validate_payload_against_contract(payload, contract)\n","\n","        # Extract metrics consistently from contract payload\n","        metrics = payload.get(\"metrics\", {}) if isinstance(payload, dict) else {}\n","        child_status = payload.get(\"status\", \"SUCCESS\")\n","        row_in = metrics.get(\"row_in\")\n","        row_out = metrics.get(\"row_out\")\n","        partition_count = metrics.get(\"partition_count\")\n","        dedup_dropped = metrics.get(\"dedup_dropped\")\n","\n","        # Duration (router-level; entity may also provide its own timing)\n","        end_ts = utc_now()\n","\n","        append_log_step({\n","            \"run_id\": run_id,\n","            \"entity_code\": entity_code_current,\n","            \"notebook_name\": cfg.get(\"notebook_name\"),\n","            \"start_ts\": step_start_ts,\n","            \"end_ts\": end_ts,\n","            \"status\": child_status,\n","            \"row_in\": row_in,\n","            \"row_out\": row_out,\n","            \"partition_count\": partition_count,\n","            \"dedup_dropped\": dedup_dropped,\n","            \"payload_json\": json.dumps(payload)\n","        })\n","\n","        # Propagate a non-success status as pipeline-visible exit (and fail fast if critical)\n","        if child_status not in (\"SUCCESS\", \"SKIPPED\"):\n","            raise ValueError(f\"Entity notebook returned non-success status: {child_status}\")\n","\n","        results.append({\"entity_code\": entity_code_current, \"status\": child_status, \"payload\": payload})\n","\n","    except Exception as e:\n","        # notebook.exit() lève une exception \"NotebookExit\" pour arrêter le notebook : c'est NORMAL.\n","        # Selon le namespace/runtime, la classe exacte peut varier; on détecte aussi par nom.\n","        if (NotebookExit is not None and isinstance(e, NotebookExit)) or _is_notebook_exit(e):\n","            raise\n","\n","        msg = safe_str(e)\n","\n","        append_log_step({\n","            \"run_id\": run_id,\n","            \"entity_code\": entity_code_current,\n","            \"notebook_name\": cfg.get(\"notebook_name\"),\n","            \"start_ts\": step_start_ts,\n","            \"end_ts\": utc_now(),\n","            \"status\": \"FAILED\",\n","            \"error_message\": msg,\n","            \"payload_json\": None\n","        })\n","\n","        any_failed = True\n","        is_critical = bool(cfg.get(\"critical\", True))\n","        any_failed_critical = any_failed_critical or is_critical\n","\n","        if is_critical:\n","            # Fail fast on critical entity\n","            raise\n","        else:\n","            results.append({\"entity_code\": entity_code_current, \"status\": \"FAILED_NONCRITICAL\", \"error\": msg})\n","            continue\n","\n","# Final exit for ALL (or single)\n","if any_failed_critical:\n","    raise ValueError(\"One or more CRITICAL entities failed in ALL run.\")\n","else:\n","    exit_notebook({\n","        \"status\": \"SUCCESS\" if not any_failed else \"SUCCESS_WITH_NONCRITICAL_FAILURES\",\n","        \"entity\": \"ALL\" if is_all else entity_code,\n","        \"results\": results\n","    })\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"88a75acd-3d24-4d49-a3fc-2fe2deb1fe0b","normalized_state":"finished","queued_time":"2025-12-19T14:28:55.3343388Z","session_start_time":null,"execution_start_time":"2025-12-19T14:28:55.555793Z","execution_finish_time":"2025-12-19T14:29:46.6174642Z","parent_msg_id":"4bf20ff7-d464-4c5f-8460-8f4092e77f87"},"text/plain":"StatementMeta(, 88a75acd-3d24-4d49-a3fc-2fe2deb1fe0b, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.mssparkutilsrunmultiple-result+json":{"activities":[{"duration":39113,"start_time":1766154540800,"session_id":"88a75acd-3d24-4d49-a3fc-2fe2deb1fe0b","created_time":1766154540800,"spark_pool":"Starter Pool","in_pipeline":false,"snapshot_status":"success","notebook_name":"nb_silver_fx","exception":"","end_time":1766154579913,"snapshot_error":"","run_id":"fb4c88d7-c1fc-4530-946e-dcd5da39c034","artifact_id":"2193bf63-3332-4d43-8fc1-913e58f6eafe","progress":100,"status":"success","status_msg":"Success","activity_name":"0","args":{"entity_code":"fx_rates","load_mode":"full","run_id":"manual-20251219T132920Z","environment":"dev","as_of_date":""},"exit_value":"{\"contract_version\": \"1.0\", \"layer\": \"silver\", \"run_id\": \"manual-20251219T132920Z\", \"entity_code\": \"fx_rates\", \"load_mode\": \"full\", \"as_of_date\": null, \"status\": \"SUCCESS\", \"metrics\": {\"row_in\": 451512, \"row_out\": 451512, \"partition_count\": 256, \"dedup_dropped\": 0}, \"table\": {\"target_table\": \"silver_fx_rates\", \"partition_cols\": [\"fx_month\"]}, \"timing\": {\"started_utc\": \"2025-12-19T14:29:08Z\", \"ended_utc\": \"2025-12-19T14:29:36Z\", \"duration_ms\": 27477}, \"quality\": {\"fail_fast_checks\": [{\"name\": \"base_currency_is_EUR\", \"passed\": true}, {\"name\": \"natural_keys_not_null\", \"passed\": true}]}, \"notes\": {\"message\": null}}","root_artifact_id":"9b0039ac-d636-403b-92fe-d0e65dca3959","capacity_id":"BEFEFEA4-F551-4DE0-A094-16B00B024FAD","workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}],"numbers":{"pending":0,"running":0,"failed":0,"succeeded":1},"limit":50}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[DEBUG nb_load_silver] Before load_runtime_contract\n[DEBUG nb_load_silver] Before normalize_entity_payload\n[DEBUG nb_load_silver] Before validate_payload_against_contract\n[DEBUG nb_load_silver] Before final SUCCESS log\nExitValue: {\"status\": \"SUCCESS\", \"entity\": \"fx_rates\", \"payload\": {\"contract_version\": \"1.0\", \"layer\": \"silver\", \"run_id\": \"manual-20251219T132920Z\", \"entity_code\": \"fx_rates\", \"load_mode\": \"full\", \"as_of_date\": null, \"status\": \"SUCCESS\", \"metrics\": {\"row_in\": 451512, \"row_out\": 451512, \"partition_count\": 256, \"dedup_dropped\": 0}, \"table\": {\"target_table\": \"silver_fx_rates\", \"partition_cols\": [\"fx_month\"]}, \"timing\": {\"started_utc\": \"2025-12-19T14:29:08Z\", \"ended_utc\": \"2025-12-19T14:29:36Z\", \"duration_ms\": 27477}, \"quality\": {\"fail_fast_checks\": [{\"name\": \"base_currency_is_EUR\", \"passed\": true}, {\"name\": \"natural_keys_not_null\", \"passed\": true}]}, \"notes\": {\"message\": null}}}"]}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"934f6d57-5203-4799-8553-a3335d560dbe"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}