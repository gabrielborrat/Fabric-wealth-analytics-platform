# Silver Layer — Notebook Documentation  
**Wealth Management Analytics Platform — Microsoft Fabric**  
**Version:** 1.0  
**Author:** Gabriel Borrat  

---

## 1. Introduction

This document provides a detailed technical description of the **Silver transformation notebooks** used in the Wealth Management Analytics Platform, specifically:

- `nb_silver_load` — the dispatcher notebook that routes to entity-specific transformations  
- Entity-specific notebooks (`nb_silver_fx`, `nb_silver_users`, `nb_silver_cards`, `nb_silver_mcc`, `nb_silver_transactions`)  
- `nb_silver_utils` — shared utility functions  
- `nb_silver_ddl` — table definition notebook  

These notebooks form the core of the Silver transformation workflow, providing data cleaning, validation, deduplication, type precision, and analytical derivations.

The design follows **banking-grade engineering** principles: deterministic transformations, strict schema control, full lineage, reproducible processing, and fail-fast validation.

---

## 2. nb_silver_load — Dispatcher Notebook

`nb_silver_load` is the central routing notebook that dispatches to entity-specific transformation notebooks based on the `entity_code` parameter.

---

### 2.1 Notebook Parameters (Fabric-Compliant Parameter Cells)

The notebook expects the following parameters, injected from the pipeline:

| Parameter | Description |
|-----------|-------------|
| `run_id` | Unique run identifier generated by pipeline. |
| `entity_code` | Entity identifier (fx_rates, users, cards, mcc, transactions). |
| `load_mode` | FULL or INCREMENTAL (incremental based on ingestion_ts or as_of_date). |
| `as_of_date` | Optional execution date for point-in-time processing (YYYY-MM-DD format). |

These parameters ensure full decoupling from pipeline logic and are mandatory for reproducibility and lineage.

---

### 2.2 High-Level Processing Flow

The notebook performs the following sequence:

1. **Parameter validation**  
   - Validates required parameters are present
   - Sets defaults if optional parameters are missing

2. **Entity dispatch**  
   - Routes to entity-specific notebook based on `entity_code`:
     - `fx_rates` → `nb_silver_fx`
     - `users` → `nb_silver_users`
     - `cards` → `nb_silver_cards`
     - `mcc` → `nb_silver_mcc`
     - `transactions` → `nb_silver_transactions`

3. **Entity notebook execution**  
   - Calls entity-specific transformation notebook
   - Passes all parameters through
   - Handles errors and returns exit payload

This pattern provides:
- clean modularity
- easy extensibility
- strict isolation of transformation logic per entity
- centralized parameter management

---

## 3. Standard Transformation Pattern

All entity-specific notebooks follow a **consistent transformation pattern** to ensure uniformity and maintainability.

### 3.1 Transformation Sequence

Each entity notebook implements this sequence:

1. **Read from Bronze**  
   - Load entity-specific Bronze table (e.g., `bronze_fx_raw`, `bronze_transaction_raw`)
   - Apply load_mode filtering (FULL vs INCREMENTAL)

2. **Structural assertions**  
   - Validate required columns exist
   - Check data types match expectations
   - Fail-fast if structure is invalid

3. **Rename and normalize**  
   - Align column names to Silver schema
   - Normalize codes (MCC codes padded to 4 digits, currency codes uppercased)
   - Standardize text fields (TRIM, UPPER where appropriate)

4. **Type casting**  
   - Enforce DECIMAL precision for financial amounts (DECIMAL(18,2) or DECIMAL(18,8) for rates)
   - Convert dates to proper DATE/TIMESTAMP types
   - Convert flags to BOOLEAN
   - Ensure integer types for counts and IDs

5. **Derive analytical columns**  
   - Date hierarchies: `txn_date`, `txn_month`, `fx_date`, `fx_month`
   - Computed flags: `is_success`, `is_active`
   - Normalized identifiers: `mcc_code`, `client_id`

6. **Add technical metadata**  
   - `source_file` — Original Bronze source file
   - `ingestion_date` — Processing date
   - `ingestion_ts` — Precise processing timestamp

7. **Generate record_hash**  
   - SHA-256 hash of key business attributes
   - Used for change detection and auditability
   - Hash includes natural key columns

8. **Deduplication**  
   - Window function to identify latest record per natural key
   - Order by `ingestion_ts` DESC
   - Keep only most recent record per natural key

9. **Project canonical schema**  
   - Strict final `select()` with exact column order
   - Matches Schema Registry contract exactly
   - No additional columns allowed

10. **Fail-fast validation**  
    - Partition cardinality checks (for partitioned tables)
    - Null constraint validation
    - Data type validation
    - Schema alignment checks

11. **Write to Silver Delta table**  
    - Append mode (no mergeSchema)
    - Partitioned write for time-series tables
    - Managed Delta table in Lakehouse

12. **Exit payload**  
    - Returns execution metadata (rows processed, status, timestamps)
    - Used for pipeline observability

---

## 4. Entity-Specific Transformation Modules

Below is a summary of each entity transformation notebook.

---

### 4.1 FX Rates — `nb_silver_fx`

**Source:** `bronze_fx_raw`  
**Target:** `silver_fx_rates`

**Transformations:**
- Normalize currency codes (UPPER, TRIM)
- Convert `rate_vs_usd` to DECIMAL(18,8) for precision
- Convert `date` to DATE type
- Derive `fx_month` (first day of month) for partitioning
- Set `base_currency` to 'USD' (standardized)
- Generate `record_hash` from (base_currency, currency, fx_date)

**Natural Key:** (base_currency, currency, fx_date)

**Partitioning:** `fx_month` (monthly)

**Deduplication:** Latest record per (base_currency, currency, fx_date) based on `ingestion_ts`

---

### 4.2 Users — `nb_silver_users`

**Source:** `bronze_user_raw`  
**Target:** `silver_users`

**Transformations:**
- Rename `id` to `client_id` for consistency
- Convert financial fields to DECIMAL(18,2): `per_capita_income`, `yearly_income`, `total_debt`
- Convert `latitude`/`longitude` to DECIMAL(9,6) for precision
- Standardize `gender` field (UPPER, TRIM)
- Generate `record_hash` from `client_id`

**Natural Key:** `client_id`

**Partitioning:** None (reference data)

**Deduplication:** Latest record per `client_id` based on `ingestion_ts`

---

### 4.3 Cards — `nb_silver_cards`

**Source:** `bronze_card_raw`  
**Target:** `silver_cards`

**Transformations:**
- Rename `id` to `card_id`
- Convert `has_chip` from STRING (YES/NO) to BOOLEAN
- Convert `card_on_dark_web` from STRING (YES/NO) to BOOLEAN
- Parse `expires` STRING to `expires_month` DATE
- Convert `credit_limit` to DECIMAL(18,2)
- Convert `acct_open_date` STRING to DATE
- Generate `record_hash` from `card_id`

**Natural Key:** `card_id`

**Partitioning:** None (reference data)

**Deduplication:** Latest record per `card_id` based on `ingestion_ts`

---

### 4.4 MCC — `nb_silver_mcc`

**Source:** `bronze_mcc_raw`  
**Target:** `silver_mcc`

**Transformations:**
- Normalize MCC code: pad to 4 digits with leading zeros (`lpad(mcc, 4, '0')`)
- Rename to `mcc_code` (STRING)
- Standardize `mcc_description` (TRIM, UPPER)
- Generate `record_hash` from `mcc_code`

**Natural Key:** `mcc_code`

**Partitioning:** None (reference data)

**Deduplication:** Latest record per `mcc_code` based on `ingestion_ts`

---

### 4.5 Transactions — `nb_silver_transactions`

**Source:** `bronze_transaction_raw`  
**Target:** `silver_transactions`

**Transformations:**
- Rename `id` to `transaction_id`
- Convert `date` TIMESTAMP to proper TIMESTAMP type
- Derive `txn_date` (DATE) from `date`
- Derive `txn_month` (DATE, first day of month) for partitioning
- Convert `amount` STRING to DECIMAL(18,2)
- Normalize `mcc` to `mcc_code` (4-digit padded STRING)
- Convert `errors` to `error_code` (INT, 0 = success)
- Derive `is_success` BOOLEAN from `error_code` (error_code == 0)
- Standardize text fields: `merchant_city`, `merchant_state`, `zip`
- Generate `record_hash` from `transaction_id`

**Natural Key:** `transaction_id`

**Partitioning:** `txn_month` (monthly)

**Deduplication:** Latest record per `transaction_id` based on `ingestion_ts`

**Special Considerations:**
- High-volume table requiring efficient partitioning
- Partition cardinality validation (max 240 distinct months)
- Foreign key relationships to users, cards, mcc

---

## 5. nb_silver_utils — Shared Utility Functions

The `nb_silver_utils` notebook contains reusable helper functions used across all Silver transformations.

### 5.1 Date Derivation Functions

**`add_txn_dates(df, ts_col="txn_ts")`**
- Derives `txn_date` (DATE) from timestamp column
- Derives `txn_month` (DATE, first day of month) for partitioning
- Used for transaction date hierarchies

### 5.2 Code Normalization Functions

**`normalize_mcc(df, col="mcc")`**
- Pads MCC code to 4 digits with leading zeros
- Converts to STRING type
- Ensures consistent MCC code format

### 5.3 Record Hash Generation

**`generate_record_hash(df, key_cols)`**
- Generates SHA-256 hash of key business attributes
- Used for change detection and auditability
- Input: DataFrame and list of key column names
- Output: DataFrame with `record_hash` column

### 5.4 Deduplication Functions

**`deduplicate_latest(df, key_cols, order_col="ingestion_ts")`**
- Window function to keep latest record per natural key
- Orders by `ingestion_ts` DESC
- Returns DataFrame with duplicates removed

---

## 6. Strict Schema Enforcement

The notebooks ensure **strict schema control** by:

- Avoiding dynamic schema inference
- Never using `mergeSchema` when writing Delta
- Applying a mandatory `select()` at the end of every transform
- Keeping transformation functions immutable and testable
- Validating schema against Schema Registry contracts

Reasons:

- Prevent schema drift
- Ensure stable contracts for Gold layer
- Comply with financial data governance standards
- Enable reliable Direct Lake Power BI connectivity

### 6.1 Schema Registry & Post-Transformation Governance

While Silver notebooks enforce strict, deterministic schemas at write time, the Silver layer is additionally governed by an explicit **Schema Registry** and a **post-transformation schema compliance validation process**.

**Schema Registry:**
- Each Silver entity has a versioned YAML contract in `GOVERNANCE/schema-registry/02-silver/`
- Defines: column names, data types, column order, natural keys, partitioning strategy
- These YAML contracts represent the **frozen schema definition** for the Silver layer

**Post-Transformation Schema Validation:**
- Schema compliance can be validated by a dedicated notebook (if implemented)
- Compares expected vs actual schemas
- Persists results into compliance tracking tables

**Design Rationale:**
- Schema validation is intentionally **decoupled** from transformation notebooks to:
  - preserve transformation stability
  - avoid partial transformation failures due to governance checks
  - provide a centralized, cross-entity compliance view
- This design supports progressive hardening toward stricter enforcement if required

As a result, the Silver layer is governed through:
- deterministic transformation notebooks
- explicit schema contracts
- continuous, auditable schema compliance monitoring

---

## 7. Writing to Silver Tables

After transformation, deduplication, and validation:

```python
df_final.write.format("delta") \
    .mode("append") \
    .partitionBy(partition_cols) \  # if partitioned
    .saveAsTable("silver_<entity>")
```

**Requirements:**

- Lakehouse binding must be active
- Silver table must exist with matching schema
- Notebook retries are enabled at pipeline level
- Partition columns must be validated (cardinality limits)

This ensures consistent transformation and error isolation.

---

## 8. Deduplication Strategy

### 8.1 Latest-Record-Wins

Silver implements **latest-record-wins** deduplication using window functions:

```python
from pyspark.sql.window import Window

window_spec = Window.partitionBy(natural_key_cols) \
    .orderBy(F.col("ingestion_ts").desc())

df_deduped = df.withColumn("rn", F.row_number().over(window_spec)) \
    .filter(F.col("rn") == 1) \
    .drop("rn")
```

**Benefits:**
- Idempotent processing (replays don't create duplicates)
- Handles incremental processing correctly
- Ensures data freshness (latest version always wins)

### 8.2 Natural Keys

Each entity defines natural keys for deduplication:

- `silver_fx_rates`: (base_currency, currency, fx_date)
- `silver_users`: (client_id)
- `silver_cards`: (card_id)
- `silver_mcc`: (mcc_code)
- `silver_transactions`: (transaction_id)

---

## 9. Partitioning Strategy

### 9.1 Time-Partitioned Tables

Tables with high volume and time-series queries are partitioned by month:

- `silver_transactions` → partitioned by `txn_month`
- `silver_fx_rates` → partitioned by `fx_month`

**Benefits:**
- Query performance for time-range filters
- Efficient incremental processing
- Optimized Direct Lake Power BI scans

**Validation:**
- Partition cardinality limits (max 240 distinct months)
- Fail-fast if threshold exceeded

### 9.2 Non-Partitioned Tables

Reference data tables (users, cards, mcc) are not partitioned:
- Lower volume
- Full-table scans acceptable
- Simpler maintenance

---

## 10. nb_silver_ddl — Table Definition Notebook

The `nb_silver_ddl` notebook contains DDL statements for creating all Silver tables.

### 10.1 Table Creation

Each table is created with:
- Exact schema matching Schema Registry contract
- Partitioning (where applicable)
- Managed Delta table format
- NOT NULL constraints on critical fields

### 10.2 Control Table

The notebook also creates `silver_ctl_entity` control table:
- Entity metadata (entity_code, enabled, load_order)
- Notebook configuration (notebook_name, notebook_path)
- Load configuration (load_mode_default, partition_cols)
- Dependency management (depends_on)
- Operational settings (critical, timeout_minutes, retries)

This table enables dynamic orchestration and entity management.

---

## 11. Extensibility of the Notebook Framework

To transform a new entity:

1. Create entity-specific notebook (e.g., `nb_silver_newentity`)
   - Follow standard transformation pattern
   - Implement deduplication logic
   - Add technical metadata
   - Write to Silver Delta table

2. Update `nb_silver_load` dispatcher
   - Add entity routing logic

3. Create Silver table DDL in `nb_silver_ddl`
   - Define schema matching Schema Registry

4. Register entity in Schema Registry
   - Create YAML contract file

5. Register entity in `silver_ctl_entity` control table
   - Add entity metadata and configuration

No changes to pipeline structure are required.

This modularity is intentional and supports enterprise-scale transformation.

---

## 12. Conclusion

The Silver notebooks create a **robust, auditable, and extensible transformation foundation** for the Wealth Management Analytics Platform. Key strengths include:

- unified transformation pattern
- strict schema enforcement
- modular entity transformations
- technical metadata injection
- predictable deduplication behavior
- strategic partitioning for performance
- clean integration with Fabric pipelines and Lakehouse

This notebook architecture ensures that all Silver datasets are **consistent, compliant, and ready for downstream Gold analytical modeling, Data Warehouse transformations, and advanced Power BI reporting**.

---
