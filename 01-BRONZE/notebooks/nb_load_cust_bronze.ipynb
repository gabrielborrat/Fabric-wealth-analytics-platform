{"cells":[{"cell_type":"code","source":["\n","# ==========================================\n","# PARAMETER CELLS (Fabric Notebook)\n","# ==========================================\n","# Ces 3 variables doivent être marquées comme \"Parameter\" dans Fabric\n","\n","LandingPath = \"Files/landing/customers/2025-12-01/churn.csv\"  # exemple, surchargé par le pipeline\n","ExecDate = \"2025-12-01\"                                    # exemple, surchargé par le pipeline\n","Entity = \"CUSTOMER\"                                         # exemple, surchargé par le pipeline\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"968dc472-0ce7-4b11-87fd-6c8506c94fa7"},{"cell_type":"code","source":["# ==========================================\n","# IMPORTS\n","# ==========================================\n","from pyspark.sql.functions import (\n","    col,\n","    lit,\n","    upper,\n","    trim,\n","    to_date,\n","    current_timestamp\n",")\n","from pyspark.sql import functions as F\n","\n","\n","# ==========================================\n","# LECTURE DU FICHIER SOURCE\n","# ==========================================\n","# On lit le CSV depuis le landing path.\n","# input_path doit être un chemin valide depuis le Lakehouse lié (ex: \"Files/landing/customers/2025-12-01/churn.csv\")\n","\n","df_raw = (\n","    spark.read\n","         .format(\"csv\")\n","         .option(\"header\", \"true\")\n","         .option(\"inferSchema\", \"true\")\n","         .load(LandingPath)\n",")\n","\n","display(df_raw.limit(10))\n","\n","\n","# ==========================================\n","# NORMALISATION / RENOMMAGE DES COLONNES\n","# ==========================================\n","# Schéma du fichier churn.csv :\n","# RowNumber, CustomerId, Surname, CreditScore, Geography, Gender,\n","# Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember,\n","# EstimatedSalary, Exited\n","\n","df_norm = (\n","    df_raw\n","        # Renommage en snake_case\n","        .withColumnRenamed(\"RowNumber\", \"row_number\")\n","        .withColumnRenamed(\"CustomerId\", \"customer_id\")\n","        .withColumnRenamed(\"Surname\", \"surname\")\n","        .withColumnRenamed(\"CreditScore\", \"credit_score\")\n","        .withColumnRenamed(\"Geography\", \"geography\")\n","        .withColumnRenamed(\"Gender\", \"gender\")\n","        .withColumnRenamed(\"Age\", \"age\")\n","        .withColumnRenamed(\"Tenure\", \"tenure\")\n","        .withColumnRenamed(\"Balance\", \"balance\")\n","        .withColumnRenamed(\"NumOfProducts\", \"num_products\")\n","        .withColumnRenamed(\"HasCrCard\", \"has_credit_card\")\n","        .withColumnRenamed(\"IsActiveMember\", \"is_active_member\")\n","        .withColumnRenamed(\"EstimatedSalary\", \"estimated_salary\")\n","        .withColumnRenamed(\"Exited\", \"exited\")\n",")\n","\n","display(df_norm.limit(10))\n","\n","\n","# ==========================================\n","# TYPAGE EXPLICITE DES COLONNES\n","# ==========================================\n","df_typed = (\n","    df_norm\n","        .withColumn(\"row_number\",        col(\"row_number\").cast(\"int\"))\n","        .withColumn(\"customer_id\",       col(\"customer_id\").cast(\"long\"))\n","        .withColumn(\"credit_score\",      col(\"credit_score\").cast(\"int\"))\n","        .withColumn(\"age\",               col(\"age\").cast(\"int\"))\n","        .withColumn(\"tenure\",            col(\"tenure\").cast(\"int\"))\n","        .withColumn(\"balance\",           col(\"balance\").cast(\"double\"))\n","        .withColumn(\"num_products\",      col(\"num_products\").cast(\"int\"))\n","        .withColumn(\"has_credit_card\",   col(\"has_credit_card\").cast(\"int\"))\n","        .withColumn(\"is_active_member\",  col(\"is_active_member\").cast(\"int\"))\n","        .withColumn(\"estimated_salary\",  col(\"estimated_salary\").cast(\"double\"))\n","        .withColumn(\"exited\",            col(\"exited\").cast(\"int\"))\n",")\n","\n","display(df_typed.limit(10))\n","\n","\n","# ==========================================\n","# NORMALISATION DES STRINGS\n","# ==========================================\n","# On nettoie et met en UPPERCASE certaines colonnes texte.\n","\n","df_clean = (\n","    df_typed\n","        .withColumn(\"surname\",   upper(trim(col(\"surname\"))))\n","        .withColumn(\"geography\", upper(trim(col(\"geography\"))))\n","        .withColumn(\"gender\",    upper(trim(col(\"gender\"))))\n",")\n","\n","\n","# ==========================================\n","# COLONNES TECHNIQUES (BRONZE)\n","# ==========================================\n","# - source_file      : permet de tracer le fichier d'origine\n","# - ingestion_date   : date d'exécution (exec_date)\n","# - ingestion_ts     : timestamp technique\n","# - entity           : type d'entité (CUSTOMER)\n","\n","df_bronze = (\n","    df_clean\n","        .withColumn(\"source_file\",   lit(LandingPath))\n","        .withColumn(\"ingestion_date\", to_date(lit(ExecDate), \"yyyy-MM-dd\"))\n","        .withColumn(\"ingestion_ts\",  current_timestamp())\n","        .withColumn(\"entity\",        lit(Entity))\n",")\n","\n","display(df_bronze.limit(10))\n","\n","\n","# ==========================================\n","# ÉCRITURE DANS LA TABLE BRONZE\n","# ==========================================\n","# IMPORTANT :\n","# - Le notebook doit être lié au Lakehouse `lh_wm_core`\n","# - La table sera créée (si besoin) en mode Delta dans ce Lakehouse.\n","\n","target_table = \"bronze_customers_raw\"\n","\n","(\n","    df_bronze\n","        .write\n","        .mode(\"append\")\n","        .format(\"delta\")\n","        .saveAsTable(target_table)\n",")\n","\n","print(f\"✅ Ingestion bronze terminée pour {entity} dans la table {target_table}\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"25884314-18a5-428c-b675-95e0ead72b9e"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}