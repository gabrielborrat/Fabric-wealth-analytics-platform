{"cells":[{"cell_type":"code","source":["# PARAMETER_CELL\n","# Ces variables seront alimentées par l'activité Notebook dans le pipeline Fabric\n","\n","entity = \"FX\"              # ex : \"FX\", \"CUSTOMER\"\n","ingestion_mode = \"INCR\"    # \"INCR\" ou \"FULL\"\n","source_path = \"\"           # ex : \"FX-rates-since2004-dataset/\" ou \"customers/\"\n","all_files_json = \"[]\"      # sortie de GetMetadata.childItems en JSON (string)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"bf0fb2c9-a840-4b15-b466-021c875fae45"},{"cell_type":"code","source":["import json\n","from pyspark.sql import functions as F\n","from pyspark.sql.utils import AnalysisException\n","from notebookutils import mssparkutils\n","\n","print(\"== nb_prepare_incremental_list v2 ==\")\n","print(f\"entity         = {entity}\")\n","print(f\"ingestion_mode = {ingestion_mode}\")\n","print(f\"source_path    = {source_path}\")\n","print(f\"all_files_json (raw, 500 chars max) = {all_files_json[:500]}\")\n","\n","# 1) Parsing brut\n","try:\n","    parsed = json.loads(all_files_json)\n","except Exception as e:\n","    print(\"ERREUR : impossible de parser all_files_json → on considère qu'il n'y a aucun fichier.\")\n","    print(str(e))\n","    mssparkutils.notebook.exit(\"[]\")\n","\n","files_list = None\n","\n","# 2) Plusieurs cas possibles selon la forme de l'output GetMetadata + @string()\n","\n","# Cas standard : c'est déjà une liste de fichiers\n","if isinstance(parsed, list):\n","    files_list = parsed\n","\n","# Cas : c'est un dict qui contient une liste\n","elif isinstance(parsed, dict):\n","    # on essaie plusieurs clés possibles, au cas où\n","    for key in [\"childItems\", \"value\", \"items\"]:\n","        if key in parsed and isinstance(parsed[key], list):\n","            files_list = parsed[key]\n","            print(f\"files_list récupéré depuis parsed['{key}']\")\n","            break\n","\n","# Si après tout ça, on n'a toujours rien → liste vide\n","if files_list is None:\n","    print(\"Aucune liste de fichiers détectée dans all_files_json → files_list = [].\")\n","    files_list = []\n","\n","print(f\"Nombre de fichiers trouvés côté source (files_list) : {len(files_list)}\")\n","\n","if len(files_list) == 0:\n","    # Rien à traiter → renvoyer une liste vide au pipeline\n","    mssparkutils.notebook.exit(\"[]\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e50e39b-4311-48ee-a615-1601a46b1cf8"},{"cell_type":"code","source":["# Cellule 3 : création du DataFrame des fichiers source\n","\n","# On crée un DataFrame à partir de la liste de dicts (name, type, size, lastModified…)\n","df_files = spark.createDataFrame(files_list)\n","\n","if \"name\" not in df_files.columns:\n","    print(\"ERREUR : la structure des fichiers ne contient pas de colonne 'name'.\")\n","    df_files.printSchema()\n","    mssparkutils.notebook.exit(\"[]\")\n","\n","# Ajout du file_path logique (clé utilisée dans le manifest)\n","df_files = df_files.withColumn(\n","    \"file_path\",\n","    F.concat(F.lit(source_path), F.col(\"name\"))\n",")\n","\n","print(\"Schéma df_files :\")\n","df_files.printSchema()\n","display(df_files.limit(10))\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c9b76eeb-403e-4e80-abbe-94875e1acfff"},{"cell_type":"code","source":["# Cellule 4 : lecture du manifest (tech_ingestion_manifest) si disponible\n","\n","has_manifest = False\n","\n","try:\n","    df_manifest = (\n","        spark.table(\"tech_ingestion_manifest\")\n","             .filter(F.col(\"entity\") == entity)\n","             .select(\"file_path\", \"last_status\", \"modified_datetime\")\n","    )\n","    has_manifest = True\n","    nb_manifest = df_manifest.count()\n","    print(f\"Table tech_ingestion_manifest trouvée pour entity='{entity}', lignes = {nb_manifest}\")\n","except AnalysisException:\n","    print(\"Table tech_ingestion_manifest introuvable → aucun filtrage incrémental possible (tout sera nouveau).\")\n","    has_manifest = False\n","\n","# Logique FULL / INCR\n","ing_mode = (ingestion_mode or \"INCR\").upper()\n","\n","if ing_mode == \"FULL\":\n","    print(\"Mode FULL : tous les fichiers source seront traités.\")\n","    df_to_process = df_files\n","\n","elif ing_mode == \"INCR\" and has_manifest:\n","    print(\"Mode INCR avec manifest disponible → left_anti join sur file_path\")\n","    # On enlève les fichiers déjà présents dans le manifest pour cette entity\n","    df_to_process = df_files.join(\n","        df_manifest.select(\"file_path\"),\n","        on=\"file_path\",\n","        how=\"left_anti\"\n","    )\n","\n","else:\n","    # Mode INCR mais pas de manifest pour cette entity → tout est nouveau\n","    print(\"Mode INCR mais manifest absent ou vide pour cette entity → tous les fichiers sont considérés comme nouveaux.\")\n","    df_to_process = df_files\n","\n","count_to_process = df_to_process.count()\n","print(f\"Nombre de fichiers à traiter (après filtrage incrémental) : {count_to_process}\")\n","\n","if count_to_process == 0:\n","    print(\"Aucun fichier à traiter après filtrage → on renvoie [].\")\n","    mssparkutils.notebook.exit(\"[]\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef194884-40aa-44b3-9c8a-f5ae843bce88"},{"cell_type":"code","source":["# On ne renvoie que les colonnes nécessaires au pipeline (name + file_path)\n","df_result = df_to_process.select(\"name\", \"file_path\")\n","\n","rows = df_result.collect()\n","result_list = [{\"name\": r[\"name\"], \"file_path\": r[\"file_path\"]} for r in rows]\n","\n","print(\"Aperçu de result_list :\")\n","print(result_list[:5])\n","\n","result_json = json.dumps(result_list)\n","\n","# Très important : on renvoie une STRING JSON\n","# qui sera lue côté pipeline via :\n","# @json(activity('Prepare_Incremental_List').output.result.exitValue)\n","mssparkutils.notebook.exit(result_json)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e82990d-7bc9-47bf-a91b-01c8d7dc017b"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}