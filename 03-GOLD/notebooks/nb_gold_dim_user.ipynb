{"cells":[{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding: utf-8\n","\n","# ============================================================\n","# nb_gold_dim_user_v1_0_final — Gold Dimension: User\n","#\n","# Contract-first + data-driven execution (banking-ready)\n","# - No runtime args / widgets\n","# - Reads execution context from gold_log_steps (latest RUNNING for this notebook)\n","# - Applies Gold YAML contract (schema/order/types/not-null/unique)\n","# - Dedup keep_latest by ingestion_ts\n","# - Idempotent rebuild (TRUNCATE + APPEND)\n","# - Returns deterministic metrics to dispatcher via exit_payload\n","# ============================================================\n","\n","# The command is not a standard IPython magic command. It is designed for use within Fabric notebooks only."],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9519da98-13c2-4b05-9cd7-3d7c9660fcb8"},{"cell_type":"code","source":["%run ./nb_gold_utils"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"95916057-1896-4483-ad93-950c17ef4115"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","import json\n","\n","# -----------------------------\n","# 0) Data-driven execution context\n","# -----------------------------\n","LOG_STEPS_TABLE = \"gold_log_steps\"\n","THIS_NOTEBOOK   = \"nb_gold_dim_user\"\n","\n","def _json_load_safe(s: str) -> dict:\n","    try:\n","        return json.loads(s) if s else {}\n","    except Exception:\n","        return {}\n","\n","def read_ctx_from_steps() -> dict:\n","    df = (\n","        spark.table(LOG_STEPS_TABLE)\n","        .filter((F.col(\"notebook_name\") == THIS_NOTEBOOK) & (F.col(\"status\") == \"RUNNING\"))\n","        .orderBy(F.col(\"start_ts\").desc())\n","        .limit(1)\n","    )\n","    rows = df.collect()\n","    if not rows:\n","        raise ValueError(\n","            f\"[{THIS_NOTEBOOK}] No RUNNING step found in {LOG_STEPS_TABLE}. \"\n","            \"Dispatcher must write RUNNING ctx before execution.\"\n","        )\n","\n","    payload = _json_load_safe(rows[0][\"payload_json\"])\n","    ctx = payload.get(\"ctx\", {})\n","    if not isinstance(ctx, dict) or not ctx:\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] RUNNING step payload_json has no ctx.\")\n","\n","    # Hard validations (banking-grade)\n","    if str(ctx.get(\"notebook_name\", \"\")).strip() != THIS_NOTEBOOK:\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.notebook_name mismatch: {ctx.get('notebook_name')}\")\n","    if str(ctx.get(\"gold_run_id\", \"\")).strip() == \"\":\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.gold_run_id missing\")\n","    if str(ctx.get(\"step_exec_id\", \"\")).strip() == \"\":\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.step_exec_id missing\")\n","    if str(ctx.get(\"entity_code\", \"\")).strip() == \"\":\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.entity_code missing\")\n","\n","    return ctx\n","\n","ctx = read_ctx_from_steps()\n","\n","gold_run_id  = normalize_run_id(ctx[\"gold_run_id\"])\n","step_exec_id = ctx[\"step_exec_id\"]\n","entity_code  = ctx[\"entity_code\"]\n","load_mode    = ctx.get(\"load_mode\", \"\")\n","as_of_date   = ctx.get(\"as_of_date\", \"\")\n","\n","print(f\"gold_run_id   = {gold_run_id}\")\n","print(f\"step_exec_id  = {step_exec_id}\")\n","print(f\"entity_code   = {entity_code}\")\n","print(f\"load_mode     = {load_mode}\")\n","print(f\"as_of_date    = {as_of_date}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"6bf38059-f344-47f1-a66f-a1289a6892c6"},{"cell_type":"code","source":["# -----------------------------\n","# 1) Tables / constants\n","# -----------------------------\n","SILVER_TABLE = \"silver_users\"\n","GOLD_TABLE   = \"gold_dim_user\"\n","\n","ENTITY       = GOLD_TABLE\n","SOURCE_TABLE = SILVER_TABLE"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"075d497b-07e9-4af3-960c-25f7e8ee9a4f"},{"cell_type":"code","source":["# -----------------------------\n","# 2) Read Silver (input sanity — minimal required)\n","# -----------------------------\n","df_silver = spark.table(SILVER_TABLE)\n","\n","INPUT_REQUIRED = [\n","    \"client_id\",\n","    \"current_age\", \"retirement_age\", \"birth_year\", \"birth_month\", \"gender\", \"address\",\n","    \"latitude\", \"longitude\", \"per_capita_income\", \"yearly_income\", \"total_debt\",\n","    \"credit_score\", \"num_credit_cards\",\n","    \"source_file\", \"ingestion_date\", \"ingestion_ts\", \"record_hash\"\n","]\n","assert_required_columns(df_silver, INPUT_REQUIRED, ctx=f\"{SILVER_TABLE} (input)\")\n","\n","df_work = df_silver.select(*[F.col(c) for c in INPUT_REQUIRED])\n","row_in = df_work.count()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a2a5bc0e-9e59-4429-95ef-702f2454069c"},{"cell_type":"code","source":["# -----------------------------\n","# 3) Dedup keep_latest (client_id by ingestion_ts)\n","# -----------------------------\n","w = Window.partitionBy(\"client_id\").orderBy(F.col(\"ingestion_ts\").desc())\n","\n","df_dedup = (\n","    df_work\n","    .withColumn(\"_rn\", F.row_number().over(w))\n","    .filter(F.col(\"_rn\") == 1)\n","    .drop(\"_rn\")\n",")\n","\n","row_after_dedup = df_dedup.count()\n","dedup_dropped = row_in - row_after_dedup\n","\n","# Banking-grade uniqueness post-dedup\n","assert_unique_key(df_dedup, [\"client_id\"], ctx=f\"{ENTITY} (post-dedup)\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1c09f7c9-9e5e-4338-83b3-239a817103ad"},{"cell_type":"code","source":["# -----------------------------\n","# 4) Add Gold technical columns\n","# -----------------------------\n","df_gold_raw = (\n","    df_dedup\n","    .withColumn(\"gold_run_id\", F.lit(gold_run_id))\n","    .withColumn(\"gold_load_ts\", F.current_timestamp())\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"84fce96f-8dc3-4387-a564-9e7f725e4ce4"},{"cell_type":"code","source":["# -----------------------------\n","# 5) Load Gold contract (YAML) + canonical projection\n","# -----------------------------\n","contract = load_gold_contract(GOLD_TABLE)  # Files/governance/gold/gold_dim_user.yaml\n","df_final = project_to_gold_contract(df_gold_raw, contract)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c67a1059-73c0-4f63-a8fb-c678b9f3faa3"},{"cell_type":"code","source":["# -----------------------------\n","# 6) Contract-first assertions (schema, types, not_null, unique)\n","# -----------------------------\n","apply_gold_contract_assertions(\n","    df=df_final,\n","    contract=contract,\n","    ctx=f\"{ENTITY} (final)\",\n","    enforce_types=True,\n","    enforce_not_null=True,\n","    enforce_unique=True\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"17313739-840c-42af-9ff9-79ea98a92b9b"},{"cell_type":"code","source":["# -----------------------------\n","# 7) Rebuild idempotent (TRUNCATE + APPEND)\n","# -----------------------------\n","rebuild_gold_table(df_final, table_name=GOLD_TABLE)\n","\n","row_out = df_final.count()\n","row_rejected = 0\n","partition_count = 0\n","anom_count = 0\n","\n","print(f\"Loaded {GOLD_TABLE} successfully. row_in={row_in}, row_out={row_out}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b6fadbb-8bd6-4e09-93b9-c0184bacf751"},{"cell_type":"code","source":["# -----------------------------\n","# 8) Exit payload (consumed by dispatcher)\n","# -----------------------------\n","def exit_payload(**kwargs):\n","    payload = {\"status\": \"SUCCESS\", **kwargs}\n","    mssparkutils.notebook.exit(json.dumps(payload, ensure_ascii=False))\n","    \n","exit_payload(\n","    status=\"SUCCESS\",\n","    gold_run_id=gold_run_id,\n","    step_exec_id=step_exec_id,\n","    entity_code=entity_code,\n","    entity=ENTITY,\n","    target_table=GOLD_TABLE,\n","    row_in=row_in,\n","    row_out=row_out,\n","    row_rejected=row_rejected,\n","    dedup_dropped=dedup_dropped,\n","    partition_count=partition_count,\n","    anom_count=anom_count\n",")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a4e3dcb1-9685-433f-843e-07cf1ac38450"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}