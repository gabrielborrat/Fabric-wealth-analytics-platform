{"cells":[{"cell_type":"code","source":["#!/usr/bin/env python\n","# coding: utf-8\n","\n","# ============================================================\n","# nb_gold_dim_card_v1_0_final â€” Gold Dimension: Card\n","#\n","# Contract-first + data-driven execution (banking-ready)\n","# - No runtime args / widgets\n","# - Reads execution context from gold_log_steps (latest RUNNING for this notebook)\n","# - Applies Gold YAML contract (schema/order/types/not-null/unique)\n","# - Dedup keep_latest by ingestion_ts\n","# - Conformance to gold_dim_user (orphan cards are excluded + logged as anomalies)\n","# - Idempotent rebuild (TRUNCATE + APPEND)\n","# ============================================================\n","\n","# The command is not a standard IPython magic command. It is designed for use within Fabric notebooks only."],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0f50802-4df7-4411-970d-9dfcc5bb0c84"},{"cell_type":"code","source":["%run ./nb_gold_utils"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"29e1ad4a-760f-46c5-b34f-28aa64a7d127"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.window import Window\n","import json\n","\n","# -----------------------------\n","# 0) Data-driven execution context\n","# -----------------------------\n","LOG_STEPS_TABLE = \"gold_log_steps\"\n","THIS_NOTEBOOK   = \"nb_gold_dim_card\"\n","\n","def _json_load_safe(s: str) -> dict:\n","    try:\n","        return json.loads(s) if s else {}\n","    except Exception:\n","        return {}\n","\n","def read_ctx_from_steps() -> dict:\n","    df = (\n","        spark.table(LOG_STEPS_TABLE)\n","        .filter((F.col(\"notebook_name\") == THIS_NOTEBOOK) & (F.col(\"status\") == \"RUNNING\"))\n","        .orderBy(F.col(\"start_ts\").desc())\n","        .limit(1)\n","    )\n","    rows = df.collect()\n","    if not rows:\n","        raise ValueError(\n","            f\"[{THIS_NOTEBOOK}] No RUNNING step found in {LOG_STEPS_TABLE}. \"\n","            \"Dispatcher must write RUNNING ctx before execution.\"\n","        )\n","\n","    payload = _json_load_safe(rows[0][\"payload_json\"])\n","    ctx = payload.get(\"ctx\", {})\n","    if not isinstance(ctx, dict) or not ctx:\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] RUNNING step payload_json has no ctx.\")\n","\n","    # Hard validations (banking-grade)\n","    if str(ctx.get(\"notebook_name\", \"\")).strip() != THIS_NOTEBOOK:\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.notebook_name mismatch: {ctx.get('notebook_name')}\")\n","    if str(ctx.get(\"gold_run_id\", \"\")).strip() == \"\":\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.gold_run_id missing\")\n","    if str(ctx.get(\"step_exec_id\", \"\")).strip() == \"\":\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.step_exec_id missing\")\n","    if str(ctx.get(\"entity_code\", \"\")).strip() == \"\":\n","        raise ValueError(f\"[{THIS_NOTEBOOK}] ctx.entity_code missing\")\n","\n","    return ctx\n","\n","ctx = read_ctx_from_steps()\n","\n","gold_run_id  = normalize_run_id(ctx[\"gold_run_id\"])\n","step_exec_id = ctx[\"step_exec_id\"]\n","entity_code  = ctx[\"entity_code\"]\n","load_mode    = ctx.get(\"load_mode\", \"\")\n","as_of_date   = ctx.get(\"as_of_date\", \"\")\n","\n","print(f\"gold_run_id   = {gold_run_id}\")\n","print(f\"step_exec_id  = {step_exec_id}\")\n","print(f\"entity_code   = {entity_code}\")\n","print(f\"load_mode     = {load_mode}\")\n","print(f\"as_of_date    = {as_of_date}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"478859e8-58e5-46d1-b1c3-b14bf3059329"},{"cell_type":"code","source":["# -----------------------------\n","# 1) Tables / constants\n","# -----------------------------\n","SILVER_TABLE    = \"silver_cards\"\n","GOLD_TABLE      = \"gold_dim_card\"\n","GOLD_USER_TABLE = \"gold_dim_user\"\n","\n","ENTITY       = GOLD_TABLE\n","SOURCE_TABLE = SILVER_TABLE"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9e9660f-55b4-4937-beb8-0cf2fa03e380"},{"cell_type":"code","source":["# -----------------------------\n","# 2) Read Silver (input sanity)\n","# -----------------------------\n","df_silver = spark.table(SILVER_TABLE)\n","\n","INPUT_REQUIRED = [\n","    \"card_id\", \"client_id\", \"card_brand\", \"card_type\", \"card_number\", \"expires_raw\",\n","    \"expires_month\", \"cvv\", \"has_chip\", \"num_cards_issued\", \"credit_limit\", \"acct_open_date\",\n","    \"year_pin_last_changed\", \"card_on_dark_web\",\n","    \"source_file\", \"ingestion_date\", \"ingestion_ts\", \"record_hash\"\n","]\n","assert_required_columns(df_silver, INPUT_REQUIRED, ctx=f\"{SILVER_TABLE}\")\n","\n","df_work = df_silver.select(*[F.col(c) for c in INPUT_REQUIRED])\n","row_in = df_work.count()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c7454a3f-c511-4fb5-9936-0dfae61a7da1"},{"cell_type":"code","source":["# -----------------------------\n","# 3) Dedup keep_latest (card_id by ingestion_ts)\n","# -----------------------------\n","w = Window.partitionBy(\"card_id\").orderBy(F.col(\"ingestion_ts\").desc())\n","\n","df_dedup = (\n","    df_work\n","    .withColumn(\"_rn\", F.row_number().over(w))\n","    .filter(F.col(\"_rn\") == 1)\n","    .drop(\"_rn\")\n",")\n","\n","row_after_dedup = df_dedup.count()\n","dedup_dropped = row_in - row_after_dedup\n","\n","# banking-grade uniqueness post-dedup\n","assert_unique_key(df_dedup, [\"card_id\"], ctx=f\"{ENTITY} (post-dedup)\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1cfcde5e-ba34-44a4-a892-a9d7e3179c13"},{"cell_type":"code","source":["# -----------------------------\n","# 4) Conformance to gold_dim_user (client_id)\n","#    - Keep rows with NULL client_id (allowed)\n","#    - Exclude orphans where client_id is not null but missing in gold_dim_user\n","#    - Log anomalies centrally (gold_anomaly_event + gold_anomaly_kpi)\n","# -----------------------------\n","df_users = (\n","    spark.table(GOLD_USER_TABLE)\n","         .select(F.col(\"client_id\").cast(\"BIGINT\").alias(\"client_id\"))\n","         .dropDuplicates()\n",")\n","\n","df_with_client = df_dedup.filter(F.col(\"client_id\").isNotNull())\n","df_null_client = df_dedup.filter(F.col(\"client_id\").isNull())\n","\n","df_ok_client, anom_orphans = split_orphans_left_anti(\n","    fact_df=df_with_client,\n","    dim_df=df_users,\n","    fact_key=\"client_id\",\n","    dim_key=\"client_id\",\n","    rule_id=\"GOLD.CARD.CONF.001\",\n","    anom_type=\"ORPHAN_DIM_USER\",\n","    entity=ENTITY,\n","    gold_run_id=gold_run_id,\n","    source_table=SOURCE_TABLE,\n","    severity=\"HIGH\",\n","    natural_key_cols_for_event=[\"card_id\", \"client_id\"]\n",")\n","\n","# Single action for orphans\n","row_rejected = anom_orphans.count()\n","if row_rejected > 0:\n","    write_anomaly_events(anom_orphans, table_name=\"gold_anomaly_event\")\n","    write_anomaly_kpis(\n","        anom_orphans,\n","        gold_run_id=gold_run_id,\n","        entity=ENTITY,\n","        table_name=\"gold_anomaly_kpi\",\n","        sample_limit=10\n","    )\n","\n","anom_count = row_rejected\n","\n","# Conformed dataset = ok clients + null client rows\n","df_conformed = df_ok_client.unionByName(df_null_client, allowMissingColumns=False)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3eae28d-81ff-4d6a-9087-bd38beb7cd5b"},{"cell_type":"code","source":["# -----------------------------\n","# 5) Add Gold technical columns\n","# -----------------------------\n","df_gold_raw = (\n","    df_conformed\n","    .withColumn(\"gold_run_id\", F.lit(gold_run_id))\n","    .withColumn(\"gold_load_ts\", F.current_timestamp())\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"04aa1436-453b-4f51-b622-1086c7aaf936"},{"cell_type":"code","source":["# -----------------------------\n","# 6) Contract load + canonical projection + assertions\n","# -----------------------------\n","contract = load_gold_contract(GOLD_TABLE)  # Files/governance/gold/gold_dim_card.yaml\n","df_final = project_to_gold_contract(df_gold_raw, contract)\n","\n","apply_gold_contract_assertions(\n","    df=df_final,\n","    contract=contract,\n","    ctx=f\"{ENTITY} (final)\",\n","    enforce_types=True,\n","    enforce_not_null=True,\n","    enforce_unique=True\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"340e979e-d1e0-417f-96c2-c7db73b8efd4"},{"cell_type":"code","source":["# -----------------------------\n","# 7) Rebuild idempotent (TRUNCATE + APPEND)\n","# -----------------------------\n","rebuild_gold_table(df_final, table_name=GOLD_TABLE)\n","\n","row_out = df_final.count()\n","partition_count = 0  # dims are not partitioned in this model\n","\n","print(f\"Loaded {GOLD_TABLE} successfully. row_in={row_in}, row_out={row_out}, row_rejected={row_rejected}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eaea375e-2017-49ae-b2d9-f5dacd8db4f9"},{"cell_type":"code","source":["# -----------------------------\n","# 8) Exit payload (consumed by dispatcher)\n","# -----------------------------\n","def exit_payload(**kwargs):\n","    payload = {\"status\": \"SUCCESS\", **kwargs}\n","    mssparkutils.notebook.exit(json.dumps(payload, ensure_ascii=False))\n","    \n","exit_payload(\n","    status=\"SUCCESS\",\n","    gold_run_id=gold_run_id,\n","    step_exec_id=step_exec_id,\n","    entity_code=entity_code,\n","    entity=ENTITY,\n","    target_table=GOLD_TABLE,\n","    row_in=row_in,\n","    row_out=row_out,\n","    row_rejected=row_rejected,\n","    dedup_dropped=dedup_dropped,\n","    partition_count=partition_count,\n","    anom_count=anom_count\n",")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"497c7773-7d23-4526-a37f-0f23a1aaf6fb"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a"}],"default_lakehouse":"86de7d18-06e4-4a37-ac0b-fbd45cf4157a","default_lakehouse_name":"lh_wm_core","default_lakehouse_workspace_id":"300f0249-d03f-436c-97fc-5b5940cc3aa3"}}},"nbformat":4,"nbformat_minor":5}