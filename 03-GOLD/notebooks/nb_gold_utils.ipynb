{"cells":[{"cell_type":"code","source":["# nb_gold_utils\n","from pyspark.sql import DataFrame\n","from pyspark.sql import functions as F\n","from pyspark.sql import types as T\n","from typing import List, Dict, Optional, Tuple\n","\n","# ------------------------------------------------------------\n","# Microsoft Fabric / OneLake filesystem access\n","# ------------------------------------------------------------\n","from notebookutils import mssparkutils\n","\n","\n","\n","# -----------------------------------------------------------------------------\n","# 0) Runtime helpers\n","# -----------------------------------------------------------------------------\n","def now_ts():\n","    return F.current_timestamp()\n","\n","def normalize_run_id(gold_run_id: Optional[str]) -> str:\n","    if gold_run_id is None or str(gold_run_id).strip() == \"\":\n","        raise ValueError(\"gold_run_id is required (non-empty).\")\n","    return str(gold_run_id).strip()\n","\n","def normalize_entity(entity: str) -> str:\n","    if entity is None or str(entity).strip() == \"\":\n","        raise ValueError(\"entity is required (non-empty).\")\n","    return str(entity).strip()\n","\n","# -----------------------------------------------------------------------------\n","# 1) Contract-first assertions\n","# -----------------------------------------------------------------------------\n","def assert_required_columns(df: DataFrame, required_cols: List[str], ctx: str = \"\") -> None:\n","    missing = [c for c in required_cols if c not in df.columns]\n","    if missing:\n","        raise ValueError(f\"[{ctx}] Missing required columns: {missing}\")\n","\n","def assert_no_additional_columns(df: DataFrame, allowed_cols: List[str], ctx: str = \"\") -> None:\n","    extra = [c for c in df.columns if c not in allowed_cols]\n","    if extra:\n","        raise ValueError(f\"[{ctx}] Additional (unexpected) columns: {extra}\")\n","\n","def assert_column_types(df: DataFrame, expected_types: Dict[str, T.DataType], ctx: str = \"\") -> None:\n","    # Spark types are not always 1:1; keep strict but realistic.\n","    schema = {f.name: f.dataType for f in df.schema.fields}\n","    mismatches = []\n","    for col, t_expected in expected_types.items():\n","        if col not in schema:\n","            mismatches.append((col, \"MISSING\", str(t_expected)))\n","        else:\n","            t_actual = schema[col]\n","            if type(t_actual) != type(t_expected):\n","                mismatches.append((col, str(t_actual), str(t_expected)))\n","    if mismatches:\n","        raise ValueError(f\"[{ctx}] Type mismatches: {mismatches}\")\n","\n","def assert_not_null(df: DataFrame, cols: List[str], ctx: str = \"\") -> None:\n","    checks = []\n","    for c in cols:\n","        checks.append(F.sum(F.when(F.col(c).isNull(), F.lit(1)).otherwise(F.lit(0))).alias(c))\n","    row = df.select(checks).collect()[0].asDict()\n","    bad = {k: v for k, v in row.items() if v and v > 0}\n","    if bad:\n","        raise ValueError(f\"[{ctx}] NOT NULL violated: {bad}\")\n","\n","def assert_unique_key(df: DataFrame, key_cols: List[str], ctx: str = \"\") -> None:\n","    if not key_cols:\n","        raise ValueError(f\"[{ctx}] key_cols is required for uniqueness check\")\n","    dup = (\n","        df.groupBy([F.col(c) for c in key_cols])\n","          .count()\n","          .filter(F.col(\"count\") > 1)\n","          .limit(1)\n","          .count()\n","    )\n","    if dup > 0:\n","        raise ValueError(f\"[{ctx}] Uniqueness violated for key: {key_cols}\")\n","\n","# -----------------------------------------------------------------------------\n","# 2) Hash helpers (stable keys)\n","# -----------------------------------------------------------------------------\n","def add_key_hash(df: DataFrame, key_cols: List[str], out_col: str = \"key_hash\") -> DataFrame:\n","    # Important: cast to string to avoid null/type issues\n","    exprs = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"∅\")) for c in key_cols]\n","    return df.withColumn(out_col, F.sha2(F.concat_ws(\"||\", *exprs), 256))\n","\n","def add_natural_keys_json(df: DataFrame, key_cols: List[str], out_col: str = \"natural_keys\") -> DataFrame:\n","    return df.withColumn(out_col, F.to_json(F.struct(*[F.col(c) for c in key_cols])))\n","\n","# -----------------------------------------------------------------------------\n","# 3) Conformance helpers\n","# -----------------------------------------------------------------------------\n","def split_orphans_left_anti(\n","    fact_df: DataFrame,\n","    dim_df: DataFrame,\n","    fact_key: str,\n","    dim_key: str,\n","    rule_id: str,\n","    anom_type: str,\n","    entity: str,\n","    gold_run_id: str,\n","    source_table: str,\n","    severity: str = \"HIGH\",\n","    anom_domain: str = \"CONFORMANCE\",\n","    natural_key_cols_for_event: Optional[List[str]] = None\n",") -> Tuple[DataFrame, DataFrame]:\n","\n","    gold_run_id = normalize_run_id(gold_run_id)\n","    entity = normalize_entity(entity)\n","\n","    f = fact_df.alias(\"f\")\n","    d = dim_df.select(F.col(dim_key).alias(dim_key)).dropDuplicates().alias(\"d\")\n","\n","    join_cond = F.col(f\"f.{fact_key}\") == F.col(f\"d.{dim_key}\")\n","\n","    # conform = left_semi, orphan = left_anti (sur condition qualifiée)\n","    conform = f.join(d, join_cond, \"left_semi\").select(\"f.*\")\n","    orphan  = f.join(d, join_cond, \"left_anti\").select(\"f.*\")\n","\n","    nk_cols = natural_key_cols_for_event or [fact_key]\n","\n","    orphan_evt = (\n","        orphan\n","        .transform(lambda x: add_key_hash(x, nk_cols, \"key_hash\"))\n","        .transform(lambda x: add_natural_keys_json(x, nk_cols, \"natural_keys\"))\n","        .withColumn(\"gold_run_id\", F.lit(gold_run_id))\n","        .withColumn(\"event_ts\", F.current_timestamp())\n","        .withColumn(\"entity\", F.lit(entity))\n","        .withColumn(\"anom_domain\", F.lit(anom_domain))\n","        .withColumn(\"anom_type\", F.lit(anom_type))\n","        .withColumn(\"severity\", F.lit(severity))\n","        .withColumn(\"rule_id\", F.lit(rule_id))\n","        .withColumn(\"source_table\", F.lit(source_table))\n","        .withColumn(\"detail\", F.lit(f\"Orphan detected: {fact_key} not found in dim ({dim_key}).\"))\n","        .withColumn(\"gold_load_ts\", F.current_timestamp())\n","        .select(\n","            \"gold_run_id\",\"event_ts\",\"entity\",\"anom_domain\",\"anom_type\",\"severity\",\"rule_id\",\n","            \"key_hash\",\"natural_keys\",\"source_table\",\"detail\",\"gold_load_ts\"\n","        )\n","    )\n","\n","    return conform, orphan_evt\n","\n","\n","# -----------------------------------------------------------------------------\n","# 4) Anomaly writers (append-only)\n","# -----------------------------------------------------------------------------\n","def write_anomaly_events(anom_df: DataFrame, table_name: str = \"gold_anomaly_event\") -> None:\n","    # Append-only, never overwrite\n","    (anom_df.write\n","        .mode(\"append\")\n","        .format(\"delta\")\n","        .saveAsTable(table_name)\n","    )\n","\n","def write_anomaly_kpis(\n","    anom_event_df: DataFrame,\n","    gold_run_id: str,\n","    entity: str,\n","    table_name: str = \"gold_anomaly_kpi\",\n","    sample_limit: int = 10\n",") -> None:\n","    gold_run_id = normalize_run_id(gold_run_id)\n","    entity = normalize_entity(entity)\n","\n","    # Aggregate counts + sample keys\n","    samples = (\n","        anom_event_df\n","        .groupBy(\"anom_domain\",\"anom_type\",\"severity\",\"rule_id\")\n","        .agg(\n","            F.count(F.lit(1)).alias(\"row_count\"),\n","            F.slice(F.collect_list(\"natural_keys\"), 1, sample_limit).alias(\"sample_keys_arr\")\n","        )\n","        .withColumn(\"sample_keys\", F.to_json(F.col(\"sample_keys_arr\")))\n","        .drop(\"sample_keys_arr\")\n","        .withColumn(\"gold_run_id\", F.lit(gold_run_id))\n","        .withColumn(\"kpi_ts\", now_ts())\n","        .withColumn(\"entity\", F.lit(entity))\n","        .withColumn(\"gold_load_ts\", now_ts())\n","        .select(\n","            \"gold_run_id\",\"kpi_ts\",\"entity\",\n","            \"anom_domain\",\"anom_type\",\"severity\",\"rule_id\",\n","            \"row_count\",\"sample_keys\",\"gold_load_ts\"\n","        )\n","    )\n","\n","    (samples.write\n","        .mode(\"append\")\n","        .format(\"delta\")\n","        .saveAsTable(table_name)\n","    )\n","\n","# -----------------------------------------------------------------------------\n","# 5) Standard write helpers (facts/dims)\n","# -----------------------------------------------------------------------------\n","def truncate_table(table_name: str) -> None:\n","    spark.sql(f\"TRUNCATE TABLE {table_name}\")\n","\n","def write_gold_table_append(df: DataFrame, table_name: str, partition_cols: Optional[List[str]] = None) -> None:\n","    writer = df.write.mode(\"append\").format(\"delta\")\n","    if partition_cols:\n","        writer = writer.partitionBy(partition_cols)\n","    writer.saveAsTable(table_name)\n","\n","def rebuild_gold_table(df: DataFrame, table_name: str, partition_cols: Optional[List[str]] = None) -> None:\n","    # Design: TRUNCATE + APPEND (idempotent)\n","    truncate_table(table_name)\n","    write_gold_table_append(df, table_name, partition_cols)\n","\n","# -----------------------------------------------------------------------------\n","# 6) Convenience metrics\n","# -----------------------------------------------------------------------------\n","def df_count(df: DataFrame) -> int:\n","    return df.count()\n","\n","def null_counts(df: DataFrame, cols: List[str]) -> Dict[str, int]:\n","    agg_exprs = [F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c) for c in cols]\n","    row = df.select(agg_exprs).collect()[0].asDict()\n","    return {k: int(v) for k, v in row.items()}\n","\n","def log_metrics_dict(metrics: Dict) -> None:\n","    # Keep it simple for now: printed; later route to a log table if needed\n","    print(metrics)\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b4dc501b-0abe-4f81-b7b4-1a2716910291"},{"cell_type":"code","source":["# ============================================================\n","# nb_gold_utils — Contract Loader + Assertions (Gold)\n","# ============================================================\n","\n","from pyspark.sql import DataFrame\n","from pyspark.sql import types as T\n","from pyspark.sql import functions as F\n","from typing import Dict, Any, List, Optional\n","import re\n","import json\n","\n","# ----------------------------\n","# YAML read helpers\n","# ----------------------------\n","def _read_text(path: str) -> str:\n","    \"\"\"\n","    Read a small YAML file from OneLake Files using Microsoft Fabric API.\n","    \"\"\"\n","    try:\n","        return mssparkutils.fs.head(path, 1024 * 1024)  # up to 1MB\n","    except Exception as e:\n","        raise FileNotFoundError(f\"Cannot read contract at path: {path}. Error: {e}\")\n","\n","\n","def _parse_yaml(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Parse YAML into a Python dict.\n","    Prefers PyYAML if available; otherwise a minimal fallback is NOT recommended.\n","    \"\"\"\n","    try:\n","        import yaml  # PyYAML (commonly available)\n","        return yaml.safe_load(text)\n","    except Exception as e:\n","        raise RuntimeError(\n","            \"YAML parsing failed. Ensure PyYAML is available in the Spark environment. \"\n","            f\"Original error: {e}\"\n","        )\n","\n","# ----------------------------\n","# Type mapping (SQL-ish → Spark)\n","# ----------------------------\n","_DEC_RE = re.compile(r\"DECIMAL\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\", re.IGNORECASE)\n","_VC_RE  = re.compile(r\"(VAR)?CHAR\\s*\\(\\s*\\d+\\s*\\)\", re.IGNORECASE)\n","\n","def _to_spark_type(type_str: str) -> T.DataType:\n","    \"\"\"\n","    Convert a contract type string into a Spark DataType.\n","    Supports: BIGINT, INT, STRING, BOOLEAN, DATE, TIMESTAMP, DOUBLE, FLOAT,\n","              DECIMAL(p,s), and common CHAR/VARCHAR forms → StringType.\n","    \"\"\"\n","    if type_str is None:\n","        return T.StringType()\n","\n","    s = type_str.strip().upper()\n","\n","    if s in (\"STRING\", \"TEXT\"):\n","        return T.StringType()\n","    if _VC_RE.match(s) or s in (\"CHAR\", \"VARCHAR\"):\n","        return T.StringType()\n","    if s in (\"BIGINT\", \"LONG\"):\n","        return T.LongType()\n","    if s in (\"INT\", \"INTEGER\"):\n","        return T.IntegerType()\n","    if s in (\"SMALLINT\",):\n","        return T.ShortType()\n","    if s in (\"TINYINT\",):\n","        return T.ByteType()\n","    if s in (\"BOOLEAN\", \"BOOL\"):\n","        return T.BooleanType()\n","    if s in (\"DATE\",):\n","        return T.DateType()\n","    if s in (\"TIMESTAMP\", \"DATETIME\"):\n","        return T.TimestampType()\n","    if s in (\"DOUBLE\",):\n","        return T.DoubleType()\n","    if s in (\"FLOAT\", \"REAL\"):\n","        return T.FloatType()\n","\n","    m = _DEC_RE.match(s)\n","    if m:\n","        p = int(m.group(1))\n","        sc = int(m.group(2))\n","        return T.DecimalType(precision=p, scale=sc)\n","\n","    # If unknown: fail fast (banking-grade)\n","    raise ValueError(f\"Unsupported type in contract: '{type_str}'\")\n","\n","def _expected_types_from_contract(contract: Dict[str, Any]) -> Dict[str, T.DataType]:\n","    cols = contract.get(\"columns\", []) or []\n","    out: Dict[str, T.DataType] = {}\n","    for c in cols:\n","        name = c[\"name\"]\n","        typ  = c.get(\"type\")\n","        out[name] = _to_spark_type(typ)\n","    return out\n","\n","def _expected_columns_from_contract(contract: Dict[str, Any]) -> List[str]:\n","    cols = contract.get(\"columns\", []) or []\n","    return [c[\"name\"] for c in cols]\n","\n","# ----------------------------\n","# Contract path conventions\n","# ----------------------------\n","def gold_contract_path(table_name: str, base_dir: str = \"Files/governance/schema_registry/gold\") -> str:\n","    \"\"\"\n","    Builds the standard contract file path.\n","    Example: gold_fact_transactions -> Files/governance/schema_registry/gold/gold_fact_transactions.yaml\n","    \"\"\"\n","    if table_name is None or str(table_name).strip() == \"\":\n","        raise ValueError(\"table_name is required\")\n","    return f\"{base_dir.rstrip('/')}/{table_name.strip()}.yaml\"\n","\n","# ----------------------------\n","# Public API: load contract\n","# ----------------------------\n","def load_gold_contract(table_name: str, base_dir: str = \"Files/governance/schema_registry/gold\") -> Dict[str, Any]:\n","    path = gold_contract_path(table_name, base_dir)\n","    txt = _read_text(path)\n","    contract = _parse_yaml(txt)\n","\n","    # Minimal normalization / sanity checks\n","    if not isinstance(contract, dict):\n","        raise ValueError(f\"Invalid contract format in {path}: expected dict at root\")\n","\n","    c_table = contract.get(\"table\")\n","    if c_table and c_table != table_name:\n","        # Contract mismatch is a real governance issue; fail fast.\n","        raise ValueError(f\"Contract table mismatch: file says '{c_table}', requested '{table_name}'\")\n","\n","    if \"columns\" not in contract or not contract[\"columns\"]:\n","        raise ValueError(f\"Contract {path} has no columns definition\")\n","\n","    return contract\n","\n","# ----------------------------\n","# Public API: apply assertions\n","# ----------------------------\n","def apply_gold_contract_assertions(\n","    df: DataFrame,\n","    contract: Dict[str, Any],\n","    ctx: Optional[str] = None,\n","    enforce_types: bool = True,\n","    enforce_not_null: bool = True,\n","    enforce_unique: bool = True\n",") -> None:\n","    \"\"\"\n","    Applies contract-first assertions on a dataframe.\n","    - required columns\n","    - no additional columns\n","    - type checks (optional)\n","    - NOT NULL (optional; from constraints.not_null)\n","    - UNIQUE (optional; from constraints.unique)\n","    \"\"\"\n","    table_name = contract.get(\"table\", \"UNKNOWN_TABLE\")\n","    context = ctx or f\"{table_name}\"\n","\n","    expected_cols = _expected_columns_from_contract(contract)\n","    assert_required_columns(df, expected_cols, ctx=context)\n","    assert_no_additional_columns(df, expected_cols, ctx=context)\n","\n","    if enforce_types:\n","        expected_types = _expected_types_from_contract(contract)\n","        assert_column_types(df, expected_types, ctx=context)\n","\n","    constraints = contract.get(\"constraints\", {}) or {}\n","\n","    if enforce_not_null:\n","        not_null_cols = constraints.get(\"not_null\", []) or []\n","        if not_null_cols:\n","            assert_not_null(df, not_null_cols, ctx=context)\n","\n","    if enforce_unique:\n","        unique_defs = constraints.get(\"unique\", []) or []\n","        # unique may be:\n","        # - [\"colA\"] (single unique key) OR\n","        # - [[\"colA\"], [\"colB\",\"colC\"]] (multiple unique constraints)\n","        if unique_defs:\n","            # normalize\n","            if isinstance(unique_defs, list) and len(unique_defs) > 0 and isinstance(unique_defs[0], str):\n","                unique_defs = [unique_defs]  # single constraint given as list of strings\n","\n","            for uq in unique_defs:\n","                if not isinstance(uq, list) or not uq:\n","                    raise ValueError(f\"[{context}] Invalid unique constraint format: {unique_defs}\")\n","                assert_unique_key(df, uq, ctx=f\"{context} UNIQUE {uq}\")\n","\n","# ----------------------------\n","# Optional helper: cast/project dataframe to contract\n","# (useful before assertions if upstream types are messy)\n","# ----------------------------\n","def project_to_gold_contract(df: DataFrame, contract: Dict[str, Any]) -> DataFrame:\n","    \"\"\"\n","    Select columns in canonical order and cast to contract types (string-based casts for Fabric stability).\n","    \"\"\"\n","    exprs = []\n","    for c in (contract.get(\"columns\", []) or []):\n","        name = c[\"name\"]\n","        typ_str = (c.get(\"type\") or \"STRING\").strip()\n","        exprs.append(F.col(name).cast(typ_str).alias(name))\n","    return df.select(*exprs)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a3f1d62b-2c9b-430a-8012-163d48968da2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}